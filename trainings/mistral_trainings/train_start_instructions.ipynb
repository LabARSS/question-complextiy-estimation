{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b0a569a-e9e2-4e20-9d6e-0cd9061d5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb  7 20:29:27 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              61W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:48:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              58W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              57W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:CB:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              63W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4d290-356d-4bf0-886d-f04b85919dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0207 20:44:29.202000 56824 site-packages/torch/distributed/run.py:793] \n",
      "W0207 20:44:29.202000 56824 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0207 20:44:29.202000 56824 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0207 20:44:29.202000 56824 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "2025-02-07 20:44:34.114139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-07 20:44:34.126813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-07 20:44:34.126811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-07 20:44:34.126811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738950274.133091   56879 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738950274.139066   56879 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738950274.145434   56880 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738950274.145434   56878 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738950274.145434   56877 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738950274.151314   56878 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1738950274.151314   56877 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1738950274.151315   56880 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-07 20:44:34.159805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-07 20:44:34.172716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-07 20:44:34.172716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-07 20:44:34.172716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|████████████████| 10/10 [00:07<00:00,  1.27it/s]\n",
      "Loading checkpoint shards: 100%|████████████████| 10/10 [00:07<00:00,  1.25it/s]\n",
      "Loading checkpoint shards: 100%|████████████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Loading checkpoint shards: 100%|████████████████| 10/10 [00:08<00:00,  1.18it/s]\n",
      "Map (num_proc=4): 100%|█████████████| 1658/1658 [00:01<00:00, 910.47 examples/s]\n",
      "Map (num_proc=4): 100%|█████████████| 1658/1658 [00:01<00:00, 841.95 examples/s]\n",
      "Map (num_proc=4): 100%|█████████████| 1658/1658 [00:01<00:00, 860.47 examples/s]\n",
      "Map (num_proc=4): 100%|█████████████| 1658/1658 [00:01<00:00, 844.16 examples/s]\n",
      "Map (num_proc=4): 100%|███████████████| 415/415 [00:01<00:00, 281.58 examples/s]\n",
      "Map (num_proc=4): 100%|███████████████| 415/415 [00:01<00:00, 249.15 examples/s]\n",
      "Map (num_proc=4): 100%|███████████████| 415/415 [00:01<00:00, 218.87 examples/s]\n",
      "Map (num_proc=4): 100%|███████████████| 415/415 [00:01<00:00, 225.75 examples/s]\n",
      "Map (num_proc=4): 100%|████████████| 2282/2282 [00:01<00:00, 1201.01 examples/s]\n",
      "Map (num_proc=4): 100%|████████████| 2282/2282 [00:02<00:00, 1110.28 examples/s]\n",
      "Map (num_proc=4): 100%|████████████| 2282/2282 [00:01<00:00, 1259.63 examples/s]\n",
      "Map (num_proc=4): 100%|████████████| 2282/2282 [00:02<00:00, 1056.94 examples/s]\n",
      "\n",
      "=== Пример входных данных ===\n",
      "Original text with special tokens:\n",
      "</s></s></s></s></s></s></s><s>[INST] Analyze the question and select the correct answer. Answer should be a single uppercase letter. Question:Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.\n",
      "Options:\n",
      "A. False, False\n",
      "B. True, True\n",
      "C. True, False\n",
      "D. False, True [/INST] Answer:C</s>\n",
      "\n",
      "Clean text:\n",
      " Analyze the question and select the correct answer. Answer should be a single uppercase letter. Question:Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.\n",
      "Options:\n",
      "A. False, False\n",
      "B. True, True\n",
      "C. True, False\n",
      "D. False, True  Answer:C\n",
      "2025-02-07 20:48:12,701 - INFO - === Пример входных данных ===\n",
      "2025-02-07 20:48:12,702 - INFO - Original text with special tokens:\n",
      "2025-02-07 20:48:12,702 - INFO - </s></s></s></s></s></s></s><s>[INST] Analyze the question and select the correct answer. Answer should be a single uppercase letter. Question:Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.\n",
      "Options:\n",
      "A. False, False\n",
      "B. True, True\n",
      "C. True, False\n",
      "D. False, True [/INST] Answer:C</s>\n",
      "2025-02-07 20:48:12,702 - INFO - Clean text:\n",
      "2025-02-07 20:48:12,702 - INFO -  Analyze the question and select the correct answer. Answer should be a single uppercase letter. Question:Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.\n",
      "Options:\n",
      "A. False, False\n",
      "B. True, True\n",
      "C. True, False\n",
      "D. False, True  Answer:C\n",
      "Epoch 1/3:   0%|      | 0/415 [00:00<?, ?it/s, acc=0.00%, loss=init, lr=1.0e-05]2025-02-07 20:48:12,706 - INFO - Starting Epoch 1/3\n",
      "NCCL version 2.21.5+cuda12.4\n",
      "Epoch 1/3:   0%|    | 0/415 [00:12<?, ?it/s, loss=3.382, acc=47.92%, lr=1.0e-05]2025-02-07 20:48:25,266 - INFO - Epoch 1, Batch 1: Loss = 3.382, Acc = 47.92%\n",
      "2025-02-07 20:48:43,234 - INFO - Log Interval: Avg Loss = 0.1384, Avg Acc = 59.97%\n",
      "Epoch 1/3:   2%| | 10/415 [00:32<13:21,  1.98s/it, loss=1.449, acc=71.25%, lr=1.2025-02-07 20:48:45,053 - INFO - Epoch 1, Batch 11: Loss = 1.449, Acc = 71.25%\n",
      "2025-02-07 20:49:02,531 - INFO - Log Interval: Avg Loss = 0.1217, Avg Acc = 64.11%\n",
      "Epoch 1/3:   5%| | 20/415 [00:51<12:48,  1.94s/it, loss=2.202, acc=58.65%, lr=1.2025-02-07 20:49:04,258 - INFO - Epoch 1, Batch 21: Loss = 2.202, Acc = 58.65%\n",
      "2025-02-07 20:49:21,020 - INFO - Log Interval: Avg Loss = 0.1066, Avg Acc = 66.55%\n",
      "Epoch 1/3:   7%| | 30/415 [01:10<12:14,  1.91s/it, loss=1.254, acc=76.33%, lr=1.2025-02-07 20:49:22,922 - INFO - Epoch 1, Batch 31: Loss = 1.254, Acc = 76.33%\n",
      "2025-02-07 20:49:39,734 - INFO - Log Interval: Avg Loss = 0.0809, Avg Acc = 71.86%\n",
      "Epoch 1/3:  10%| | 40/415 [01:28<11:49,  1.89s/it, loss=2.052, acc=61.98%, lr=1.2025-02-07 20:49:41,563 - INFO - Epoch 1, Batch 41: Loss = 2.052, Acc = 61.98%\n",
      "2025-02-07 20:49:58,301 - INFO - Log Interval: Avg Loss = 0.0819, Avg Acc = 71.33%\n",
      "Epoch 1/3:  12%| | 50/415 [01:47<11:26,  1.88s/it, loss=0.914, acc=76.56%, lr=1.2025-02-07 20:50:00,193 - INFO - Epoch 1, Batch 51: Loss = 0.914, Acc = 76.56%\n",
      "2025-02-07 20:50:18,016 - INFO - Log Interval: Avg Loss = 0.0766, Avg Acc = 72.11%\n",
      "Epoch 1/3:  14%|▏| 60/415 [02:07<11:17,  1.91s/it, loss=1.129, acc=72.76%, lr=1.2025-02-07 20:50:19,822 - INFO - Epoch 1, Batch 61: Loss = 1.129, Acc = 72.76%\n",
      "2025-02-07 20:50:36,709 - INFO - Log Interval: Avg Loss = 0.0722, Avg Acc = 72.87%\n",
      "Epoch 1/3:  17%|▏| 70/415 [02:25<10:53,  1.89s/it, loss=1.049, acc=79.38%, lr=1.2025-02-07 20:50:38,433 - INFO - Epoch 1, Batch 71: Loss = 1.049, Acc = 79.38%\n",
      "2025-02-07 20:50:55,497 - INFO - Log Interval: Avg Loss = 0.0680, Avg Acc = 74.11%\n",
      "Epoch 1/3:  19%|▏| 80/415 [02:44<10:32,  1.89s/it, loss=1.228, acc=76.25%, lr=1.2025-02-07 20:50:57,202 - INFO - Epoch 1, Batch 81: Loss = 1.228, Acc = 76.25%\n",
      "2025-02-07 20:51:13,925 - INFO - Log Interval: Avg Loss = 0.0779, Avg Acc = 72.17%\n",
      "Epoch 1/3:  22%|▏| 90/415 [03:03<10:10,  1.88s/it, loss=1.294, acc=65.92%, lr=1.2025-02-07 20:51:15,758 - INFO - Epoch 1, Batch 91: Loss = 1.294, Acc = 65.92%\n",
      "2025-02-07 20:51:32,587 - INFO - Log Interval: Avg Loss = 0.0762, Avg Acc = 71.69%\n",
      "Epoch 1/3:  24%|▏| 100/415 [03:21<09:50,  1.88s/it, loss=1.110, acc=77.78%, lr=12025-02-07 20:51:34,477 - INFO - Epoch 1, Batch 101: Loss = 1.110, Acc = 77.78%\n",
      "2025-02-07 20:51:51,656 - INFO - Log Interval: Avg Loss = 0.0665, Avg Acc = 75.66%\n",
      "Epoch 1/3:  27%|▎| 110/415 [03:40<09:33,  1.88s/it, loss=0.665, acc=81.89%, lr=12025-02-07 20:51:53,398 - INFO - Epoch 1, Batch 111: Loss = 0.665, Acc = 81.89%\n",
      "2025-02-07 20:52:10,340 - INFO - Log Interval: Avg Loss = 0.0711, Avg Acc = 73.32%\n",
      "Epoch 1/3:  29%|▎| 120/415 [03:59<09:14,  1.88s/it, loss=1.869, acc=63.37%, lr=12025-02-07 20:52:12,158 - INFO - Epoch 1, Batch 121: Loss = 1.869, Acc = 63.37%\n",
      "2025-02-07 20:52:28,780 - INFO - Log Interval: Avg Loss = 0.0706, Avg Acc = 73.01%\n",
      "Epoch 1/3:  31%|▎| 130/415 [04:17<08:52,  1.87s/it, loss=2.306, acc=49.63%, lr=12025-02-07 20:52:30,586 - INFO - Epoch 1, Batch 131: Loss = 2.306, Acc = 49.63%\n",
      "2025-02-07 20:52:47,368 - INFO - Log Interval: Avg Loss = 0.0707, Avg Acc = 72.62%\n",
      "Epoch 1/3:  34%|▎| 140/415 [04:36<08:33,  1.87s/it, loss=1.283, acc=66.47%, lr=12025-02-07 20:52:49,271 - INFO - Epoch 1, Batch 141: Loss = 1.283, Acc = 66.47%\n",
      "2025-02-07 20:53:06,033 - INFO - Log Interval: Avg Loss = 0.0661, Avg Acc = 74.22%\n",
      "Epoch 1/3:  36%|▎| 150/415 [04:55<08:15,  1.87s/it, loss=1.049, acc=71.75%, lr=12025-02-07 20:53:08,046 - INFO - Epoch 1, Batch 151: Loss = 1.049, Acc = 71.75%\n",
      "2025-02-07 20:53:24,704 - INFO - Log Interval: Avg Loss = 0.0686, Avg Acc = 74.22%\n",
      "Epoch 1/3:  39%|▍| 160/415 [05:13<07:55,  1.87s/it, loss=1.328, acc=70.15%, lr=12025-02-07 20:53:26,588 - INFO - Epoch 1, Batch 161: Loss = 1.328, Acc = 70.15%\n",
      "2025-02-07 20:53:43,364 - INFO - Log Interval: Avg Loss = 0.0660, Avg Acc = 74.18%\n",
      "Epoch 1/3:  41%|▍| 170/415 [05:32<07:37,  1.87s/it, loss=1.697, acc=71.70%, lr=12025-02-07 20:53:45,247 - INFO - Epoch 1, Batch 171: Loss = 1.697, Acc = 71.70%\n",
      "2025-02-07 20:54:02,158 - INFO - Log Interval: Avg Loss = 0.0676, Avg Acc = 73.71%\n",
      "Epoch 1/3:  43%|▍| 180/415 [05:51<07:18,  1.87s/it, loss=0.540, acc=85.51%, lr=12025-02-07 20:54:03,888 - INFO - Epoch 1, Batch 181: Loss = 0.540, Acc = 85.51%\n",
      "2025-02-07 20:54:20,577 - INFO - Log Interval: Avg Loss = 0.0679, Avg Acc = 73.96%\n",
      "Epoch 1/3:  46%|▍| 190/415 [06:09<06:58,  1.86s/it, loss=1.351, acc=72.86%, lr=12025-02-07 20:54:22,295 - INFO - Epoch 1, Batch 191: Loss = 1.351, Acc = 72.86%\n",
      "2025-02-07 20:54:39,402 - INFO - Log Interval: Avg Loss = 0.0656, Avg Acc = 74.04%\n",
      "Epoch 1/3:  48%|▍| 200/415 [06:28<06:41,  1.87s/it, loss=1.086, acc=77.86%, lr=12025-02-07 20:54:41,135 - INFO - Epoch 1, Batch 201: Loss = 1.086, Acc = 77.86%\n",
      "2025-02-07 20:54:58,492 - INFO - Log Interval: Avg Loss = 0.0651, Avg Acc = 74.68%\n",
      "Epoch 1/3:  51%|▌| 210/415 [06:47<06:25,  1.88s/it, loss=0.883, acc=76.09%, lr=12025-02-07 20:55:00,225 - INFO - Epoch 1, Batch 211: Loss = 0.883, Acc = 76.09%\n",
      "2025-02-07 20:55:17,230 - INFO - Log Interval: Avg Loss = 0.0653, Avg Acc = 73.49%\n",
      "Epoch 1/3:  53%|▌| 220/415 [07:06<06:06,  1.88s/it, loss=1.017, acc=78.30%, lr=12025-02-07 20:55:19,011 - INFO - Epoch 1, Batch 221: Loss = 1.017, Acc = 78.30%\n",
      "2025-02-07 20:55:36,092 - INFO - Log Interval: Avg Loss = 0.0662, Avg Acc = 73.95%\n",
      "Epoch 1/3:  55%|▌| 230/415 [07:25<05:48,  1.88s/it, loss=1.357, acc=64.12%, lr=12025-02-07 20:55:37,903 - INFO - Epoch 1, Batch 231: Loss = 1.357, Acc = 64.12%\n",
      "2025-02-07 20:55:54,106 - INFO - Log Interval: Avg Loss = 0.0718, Avg Acc = 72.68%\n",
      "Epoch 1/3:  58%|▌| 240/415 [07:43<05:24,  1.85s/it, loss=1.071, acc=72.20%, lr=12025-02-07 20:55:55,825 - INFO - Epoch 1, Batch 241: Loss = 1.071, Acc = 72.20%\n",
      "2025-02-07 20:56:12,128 - INFO - Log Interval: Avg Loss = 0.0665, Avg Acc = 74.35%\n",
      "Epoch 1/3:  60%|▌| 250/415 [08:01<05:03,  1.84s/it, loss=1.367, acc=73.94%, lr=12025-02-07 20:56:13,865 - INFO - Epoch 1, Batch 251: Loss = 1.367, Acc = 73.94%\n",
      "2025-02-07 20:56:30,037 - INFO - Log Interval: Avg Loss = 0.0666, Avg Acc = 75.24%\n",
      "Epoch 1/3:  63%|▋| 260/415 [08:19<04:43,  1.83s/it, loss=1.106, acc=71.76%, lr=12025-02-07 20:56:31,956 - INFO - Epoch 1, Batch 261: Loss = 1.106, Acc = 71.76%\n",
      "2025-02-07 20:56:48,371 - INFO - Log Interval: Avg Loss = 0.0598, Avg Acc = 76.01%\n",
      "Epoch 1/3:  65%|▋| 270/415 [08:37<04:25,  1.83s/it, loss=1.028, acc=76.56%, lr=12025-02-07 20:56:50,181 - INFO - Epoch 1, Batch 271: Loss = 1.028, Acc = 76.56%\n",
      "2025-02-07 20:57:06,453 - INFO - Log Interval: Avg Loss = 0.0600, Avg Acc = 76.95%\n",
      "Epoch 1/3:  67%|▋| 280/415 [08:55<04:07,  1.83s/it, loss=0.820, acc=76.26%, lr=12025-02-07 20:57:08,607 - INFO - Epoch 1, Batch 281: Loss = 0.820, Acc = 76.26%\n",
      "2025-02-07 20:57:24,884 - INFO - Log Interval: Avg Loss = 0.0630, Avg Acc = 75.11%\n",
      "Epoch 1/3:  70%|▋| 290/415 [09:14<03:48,  1.83s/it, loss=0.601, acc=84.54%, lr=12025-02-07 20:57:26,914 - INFO - Epoch 1, Batch 291: Loss = 0.601, Acc = 84.54%\n",
      "2025-02-07 20:57:43,744 - INFO - Log Interval: Avg Loss = 0.0630, Avg Acc = 75.25%\n",
      "Epoch 1/3:  72%|▋| 300/415 [09:32<03:31,  1.84s/it, loss=1.450, acc=71.43%, lr=12025-02-07 20:57:45,540 - INFO - Epoch 1, Batch 301: Loss = 1.450, Acc = 71.43%\n",
      "2025-02-07 20:58:02,445 - INFO - Log Interval: Avg Loss = 0.0631, Avg Acc = 75.90%\n",
      "Epoch 1/3:  75%|▋| 310/415 [09:51<03:15,  1.86s/it, loss=0.361, acc=91.41%, lr=12025-02-07 20:58:04,598 - INFO - Epoch 1, Batch 311: Loss = 0.361, Acc = 91.41%\n",
      "2025-02-07 20:58:21,365 - INFO - Log Interval: Avg Loss = 0.0569, Avg Acc = 78.65%\n",
      "Epoch 1/3:  77%|▊| 320/415 [10:10<02:56,  1.86s/it, loss=0.844, acc=75.00%, lr=12025-02-07 20:58:23,234 - INFO - Epoch 1, Batch 321: Loss = 0.844, Acc = 75.00%\n",
      "2025-02-07 20:58:39,421 - INFO - Log Interval: Avg Loss = 0.0617, Avg Acc = 76.00%\n",
      "Epoch 1/3:  80%|▊| 330/415 [10:28<02:37,  1.85s/it, loss=0.373, acc=88.92%, lr=12025-02-07 20:58:41,568 - INFO - Epoch 1, Batch 331: Loss = 0.373, Acc = 88.92%\n",
      "2025-02-07 20:58:58,839 - INFO - Log Interval: Avg Loss = 0.0611, Avg Acc = 75.69%\n",
      "Epoch 1/3:  82%|▊| 340/415 [10:48<02:20,  1.88s/it, loss=1.271, acc=74.36%, lr=12025-02-07 20:59:00,958 - INFO - Epoch 1, Batch 341: Loss = 1.271, Acc = 74.36%\n",
      "2025-02-07 20:59:17,867 - INFO - Log Interval: Avg Loss = 0.0599, Avg Acc = 75.18%\n",
      "Epoch 1/3:  84%|▊| 350/415 [11:07<02:02,  1.88s/it, loss=1.490, acc=66.77%, lr=12025-02-07 20:59:19,765 - INFO - Epoch 1, Batch 351: Loss = 1.490, Acc = 66.77%\n",
      "2025-02-07 20:59:36,008 - INFO - Log Interval: Avg Loss = 0.0608, Avg Acc = 75.96%\n",
      "Epoch 1/3:  87%|▊| 360/415 [11:25<01:42,  1.86s/it, loss=0.830, acc=77.80%, lr=12025-02-07 20:59:38,026 - INFO - Epoch 1, Batch 361: Loss = 0.830, Acc = 77.80%\n",
      "2025-02-07 20:59:54,708 - INFO - Log Interval: Avg Loss = 0.0638, Avg Acc = 74.87%\n",
      "Epoch 1/3:  89%|▉| 370/415 [11:43<01:23,  1.86s/it, loss=1.447, acc=65.96%, lr=12025-02-07 20:59:56,620 - INFO - Epoch 1, Batch 371: Loss = 1.447, Acc = 65.96%\n",
      "2025-02-07 21:00:13,173 - INFO - Log Interval: Avg Loss = 0.0652, Avg Acc = 75.06%\n",
      "Epoch 1/3:  92%|▉| 380/415 [12:02<01:04,  1.85s/it, loss=0.587, acc=85.19%, lr=12025-02-07 21:00:14,997 - INFO - Epoch 1, Batch 381: Loss = 0.587, Acc = 85.19%\n",
      "2025-02-07 21:00:32,363 - INFO - Log Interval: Avg Loss = 0.0557, Avg Acc = 77.60%\n",
      "Epoch 1/3:  94%|▉| 390/415 [12:21<00:46,  1.87s/it, loss=0.686, acc=80.17%, lr=12025-02-07 21:00:34,104 - INFO - Epoch 1, Batch 391: Loss = 0.686, Acc = 80.17%\n",
      "2025-02-07 21:00:51,432 - INFO - Log Interval: Avg Loss = 0.0573, Avg Acc = 76.91%\n",
      "Epoch 1/3:  96%|▉| 400/415 [12:40<00:28,  1.89s/it, loss=0.530, acc=89.29%, lr=12025-02-07 21:00:53,574 - INFO - Epoch 1, Batch 401: Loss = 0.530, Acc = 89.29%\n",
      "2025-02-07 21:01:09,607 - INFO - Log Interval: Avg Loss = 0.0620, Avg Acc = 75.81%\n",
      "Epoch 1/3:  99%|▉| 410/415 [12:58<00:09,  1.86s/it, loss=0.859, acc=78.38%, lr=12025-02-07 21:01:11,523 - INFO - Epoch 1, Batch 411: Loss = 0.859, Acc = 78.38%\n",
      "Epoch 1/3: 100%|▉| 414/415 [13:05<00:01,  1.85s/it, loss=0.999, acc=77.56%, lr=12025-02-07 21:01:18,602 - INFO - Epoch 1, Batch 415: Loss = 0.999, Acc = 77.56%\n",
      "Epoch 1/3: 100%|▉| 414/415 [13:05<00:01,  1.90s/it, loss=0.999, acc=77.56%, lr=1\n",
      "2025-02-07 21:01:18,667 - INFO - Epoch 1 completed.\n",
      "2025-02-07 21:02:02,441 - INFO - Validation Results for Epoch 1: Loss = 0.9636, Accuracy = 76.02%\n",
      "\n",
      "Validation Results for Epoch 1:\n",
      "Loss: 0.9636\n",
      "Accuracy: 76.02%\n",
      "Epoch 2/3:   0%|      | 0/415 [00:00<?, ?it/s, acc=0.00%, loss=init, lr=1.0e-05]2025-02-07 21:02:02,629 - INFO - Starting Epoch 2/3\n",
      "Epoch 2/3:   0%|    | 0/415 [00:03<?, ?it/s, loss=0.414, acc=86.83%, lr=1.0e-05]2025-02-07 21:02:06,193 - INFO - Epoch 2, Batch 1: Loss = 0.414, Acc = 86.83%\n",
      "2025-02-07 21:02:26,421 - INFO - Log Interval: Avg Loss = 0.0360, Avg Acc = 85.20%\n",
      "Epoch 2/3:   2%| | 10/415 [00:25<14:49,  2.20s/it, loss=0.378, acc=88.17%, lr=1.2025-02-07 21:02:28,159 - INFO - Epoch 2, Batch 11: Loss = 0.378, Acc = 88.17%\n",
      "2025-02-07 21:02:44,798 - INFO - Log Interval: Avg Loss = 0.0330, Avg Acc = 85.98%\n",
      "Epoch 2/3:   5%| | 20/415 [00:44<13:14,  2.01s/it, loss=0.393, acc=87.78%, lr=1.2025-02-07 21:02:46,971 - INFO - Epoch 2, Batch 21: Loss = 0.393, Acc = 87.78%\n",
      "2025-02-07 21:03:03,511 - INFO - Log Interval: Avg Loss = 0.0391, Avg Acc = 83.63%\n",
      "Epoch 2/3:   7%| | 30/415 [01:02<12:25,  1.94s/it, loss=0.413, acc=88.24%, lr=1.2025-02-07 21:03:05,439 - INFO - Epoch 2, Batch 31: Loss = 0.413, Acc = 88.24%\n",
      "2025-02-07 21:03:22,484 - INFO - Log Interval: Avg Loss = 0.0344, Avg Acc = 85.15%\n",
      "Epoch 2/3:  10%| | 40/415 [01:21<11:58,  1.92s/it, loss=0.346, acc=89.18%, lr=1.2025-02-07 21:03:24,271 - INFO - Epoch 2, Batch 41: Loss = 0.346, Acc = 89.18%\n",
      "2025-02-07 21:03:40,836 - INFO - Log Interval: Avg Loss = 0.0358, Avg Acc = 84.37%\n",
      "Epoch 2/3:  12%| | 50/415 [01:40<11:31,  1.90s/it, loss=0.519, acc=82.19%, lr=1.2025-02-07 21:03:42,865 - INFO - Epoch 2, Batch 51: Loss = 0.519, Acc = 82.19%\n",
      "2025-02-07 21:03:59,154 - INFO - Log Interval: Avg Loss = 0.0323, Avg Acc = 85.17%\n",
      "Epoch 2/3:  14%|▏| 60/415 [01:58<11:02,  1.87s/it, loss=0.556, acc=86.21%, lr=1.2025-02-07 21:04:00,942 - INFO - Epoch 2, Batch 61: Loss = 0.556, Acc = 86.21%\n",
      "2025-02-07 21:04:17,531 - INFO - Log Interval: Avg Loss = 0.0339, Avg Acc = 84.83%\n",
      "Epoch 2/3:  17%|▏| 70/415 [02:16<10:41,  1.86s/it, loss=0.437, acc=85.71%, lr=1.2025-02-07 21:04:19,438 - INFO - Epoch 2, Batch 71: Loss = 0.437, Acc = 85.71%\n",
      "2025-02-07 21:04:36,331 - INFO - Log Interval: Avg Loss = 0.0318, Avg Acc = 85.53%\n",
      "Epoch 2/3:  19%|▏| 80/415 [02:35<10:23,  1.86s/it, loss=0.347, acc=90.60%, lr=1.2025-02-07 21:04:38,105 - INFO - Epoch 2, Batch 81: Loss = 0.347, Acc = 90.60%\n",
      "2025-02-07 21:04:54,805 - INFO - Log Interval: Avg Loss = 0.0313, Avg Acc = 85.31%\n",
      "Epoch 2/3:  22%|▏| 90/415 [02:54<10:04,  1.86s/it, loss=0.481, acc=86.91%, lr=1.2025-02-07 21:04:56,692 - INFO - Epoch 2, Batch 91: Loss = 0.481, Acc = 86.91%\n",
      "2025-02-07 21:05:14,726 - INFO - Log Interval: Avg Loss = 0.0294, Avg Acc = 86.90%\n",
      "Epoch 2/3:  24%|▏| 100/415 [03:13<09:58,  1.90s/it, loss=0.548, acc=86.12%, lr=12025-02-07 21:05:16,537 - INFO - Epoch 2, Batch 101: Loss = 0.548, Acc = 86.12%\n",
      "2025-02-07 21:05:33,250 - INFO - Log Interval: Avg Loss = 0.0321, Avg Acc = 85.03%\n",
      "Epoch 2/3:  27%|▎| 110/415 [03:32<09:35,  1.89s/it, loss=0.416, acc=85.27%, lr=12025-02-07 21:05:35,080 - INFO - Epoch 2, Batch 111: Loss = 0.416, Acc = 85.27%\n",
      "2025-02-07 21:05:52,457 - INFO - Log Interval: Avg Loss = 0.0335, Avg Acc = 84.89%\n",
      "Epoch 2/3:  29%|▎| 120/415 [03:51<09:19,  1.90s/it, loss=0.353, acc=91.78%, lr=12025-02-07 21:05:54,348 - INFO - Epoch 2, Batch 121: Loss = 0.353, Acc = 91.78%\n",
      "2025-02-07 21:06:11,357 - INFO - Log Interval: Avg Loss = 0.0333, Avg Acc = 84.24%\n",
      "Epoch 2/3:  31%|▎| 130/415 [04:10<08:59,  1.89s/it, loss=0.262, acc=93.60%, lr=12025-02-07 21:06:13,172 - INFO - Epoch 2, Batch 131: Loss = 0.262, Acc = 93.60%\n",
      "2025-02-07 21:06:29,954 - INFO - Log Interval: Avg Loss = 0.0340, Avg Acc = 85.74%\n",
      "Epoch 2/3:  34%|▎| 140/415 [04:29<08:38,  1.88s/it, loss=0.313, acc=90.61%, lr=12025-02-07 21:06:31,789 - INFO - Epoch 2, Batch 141: Loss = 0.313, Acc = 90.61%\n",
      "2025-02-07 21:06:49,473 - INFO - Log Interval: Avg Loss = 0.0291, Avg Acc = 86.04%\n",
      "Epoch 2/3:  36%|▎| 150/415 [04:48<08:23,  1.90s/it, loss=0.451, acc=87.08%, lr=12025-02-07 21:06:51,219 - INFO - Epoch 2, Batch 151: Loss = 0.451, Acc = 87.08%\n",
      "2025-02-07 21:07:07,961 - INFO - Log Interval: Avg Loss = 0.0291, Avg Acc = 86.84%\n",
      "Epoch 2/3:  39%|▍| 160/415 [05:07<08:01,  1.89s/it, loss=0.299, acc=93.23%, lr=12025-02-07 21:07:09,770 - INFO - Epoch 2, Batch 161: Loss = 0.299, Acc = 93.23%\n",
      "2025-02-07 21:07:26,990 - INFO - Log Interval: Avg Loss = 0.0307, Avg Acc = 86.09%\n",
      "Epoch 2/3:  41%|▍| 170/415 [05:26<07:43,  1.89s/it, loss=0.585, acc=82.44%, lr=12025-02-07 21:07:28,803 - INFO - Epoch 2, Batch 171: Loss = 0.585, Acc = 82.44%\n",
      "2025-02-07 21:07:45,736 - INFO - Log Interval: Avg Loss = 0.0353, Avg Acc = 84.11%\n",
      "Epoch 2/3:  43%|▍| 180/415 [05:44<07:23,  1.89s/it, loss=0.375, acc=88.41%, lr=12025-02-07 21:07:47,608 - INFO - Epoch 2, Batch 181: Loss = 0.375, Acc = 88.41%\n",
      "2025-02-07 21:08:04,660 - INFO - Log Interval: Avg Loss = 0.0344, Avg Acc = 84.47%\n",
      "Epoch 2/3:  46%|▍| 190/415 [06:03<07:05,  1.89s/it, loss=0.231, acc=95.12%, lr=12025-02-07 21:08:06,588 - INFO - Epoch 2, Batch 191: Loss = 0.231, Acc = 95.12%\n",
      "2025-02-07 21:08:23,267 - INFO - Log Interval: Avg Loss = 0.0296, Avg Acc = 86.50%\n",
      "Epoch 2/3:  48%|▍| 200/415 [06:22<06:44,  1.88s/it, loss=0.500, acc=86.29%, lr=12025-02-07 21:08:25,095 - INFO - Epoch 2, Batch 201: Loss = 0.500, Acc = 86.29%\n",
      "2025-02-07 21:08:41,660 - INFO - Log Interval: Avg Loss = 0.0288, Avg Acc = 86.22%\n",
      "Epoch 2/3:  51%|▌| 210/415 [06:40<06:22,  1.86s/it, loss=0.783, acc=82.73%, lr=12025-02-07 21:08:43,392 - INFO - Epoch 2, Batch 211: Loss = 0.783, Acc = 82.73%\n",
      "2025-02-07 21:09:00,463 - INFO - Log Interval: Avg Loss = 0.0296, Avg Acc = 86.66%\n",
      "Epoch 2/3:  53%|▌| 220/415 [06:59<06:04,  1.87s/it, loss=0.270, acc=91.78%, lr=12025-02-07 21:09:02,214 - INFO - Epoch 2, Batch 221: Loss = 0.270, Acc = 91.78%\n",
      "2025-02-07 21:09:18,714 - INFO - Log Interval: Avg Loss = 0.0291, Avg Acc = 86.02%\n",
      "Epoch 2/3:  55%|▌| 230/415 [07:17<05:43,  1.86s/it, loss=0.302, acc=89.60%, lr=12025-02-07 21:09:20,512 - INFO - Epoch 2, Batch 231: Loss = 0.302, Acc = 89.60%\n",
      "2025-02-07 21:09:37,478 - INFO - Log Interval: Avg Loss = 0.0295, Avg Acc = 86.65%\n",
      "Epoch 2/3:  58%|▌| 240/415 [07:36<05:25,  1.86s/it, loss=0.254, acc=92.86%, lr=12025-02-07 21:09:39,196 - INFO - Epoch 2, Batch 241: Loss = 0.254, Acc = 92.86%\n",
      "2025-02-07 21:09:55,968 - INFO - Log Interval: Avg Loss = 0.0300, Avg Acc = 86.14%\n",
      "Epoch 2/3:  60%|▌| 250/415 [07:55<05:06,  1.86s/it, loss=0.398, acc=89.05%, lr=12025-02-07 21:09:57,789 - INFO - Epoch 2, Batch 251: Loss = 0.398, Acc = 89.05%\n",
      "2025-02-07 21:10:14,070 - INFO - Log Interval: Avg Loss = 0.0296, Avg Acc = 86.34%\n",
      "Epoch 2/3:  63%|▋| 260/415 [08:13<04:45,  1.84s/it, loss=0.304, acc=89.71%, lr=12025-02-07 21:10:15,810 - INFO - Epoch 2, Batch 261: Loss = 0.304, Acc = 89.71%\n",
      "2025-02-07 21:10:32,768 - INFO - Log Interval: Avg Loss = 0.0308, Avg Acc = 86.24%\n",
      "Epoch 2/3:  65%|▋| 270/415 [08:31<04:28,  1.85s/it, loss=0.467, acc=85.96%, lr=12025-02-07 21:10:34,587 - INFO - Epoch 2, Batch 271: Loss = 0.467, Acc = 85.96%\n",
      "2025-02-07 21:10:50,907 - INFO - Log Interval: Avg Loss = 0.0290, Avg Acc = 86.48%\n",
      "Epoch 2/3:  67%|▋| 280/415 [08:50<04:08,  1.84s/it, loss=0.562, acc=83.33%, lr=12025-02-07 21:10:52,702 - INFO - Epoch 2, Batch 281: Loss = 0.562, Acc = 83.33%\n",
      "2025-02-07 21:11:09,590 - INFO - Log Interval: Avg Loss = 0.0320, Avg Acc = 85.11%\n",
      "Epoch 2/3:  70%|▋| 290/415 [09:09<03:52,  1.86s/it, loss=0.305, acc=90.55%, lr=12025-02-07 21:11:11,755 - INFO - Epoch 2, Batch 291: Loss = 0.305, Acc = 90.55%\n",
      "2025-02-07 21:11:29,146 - INFO - Log Interval: Avg Loss = 0.0280, Avg Acc = 86.91%\n",
      "Epoch 2/3:  72%|▋| 300/415 [09:28<03:35,  1.88s/it, loss=0.421, acc=88.36%, lr=12025-02-07 21:11:30,886 - INFO - Epoch 2, Batch 301: Loss = 0.421, Acc = 88.36%\n",
      "2025-02-07 21:11:47,772 - INFO - Log Interval: Avg Loss = 0.0269, Avg Acc = 86.98%\n",
      "Epoch 2/3:  75%|▋| 310/415 [09:46<03:16,  1.87s/it, loss=0.321, acc=90.37%, lr=12025-02-07 21:11:49,609 - INFO - Epoch 2, Batch 311: Loss = 0.321, Acc = 90.37%\n",
      "2025-02-07 21:12:06,820 - INFO - Log Interval: Avg Loss = 0.0270, Avg Acc = 87.32%\n",
      "Epoch 2/3:  77%|▊| 320/415 [10:05<02:58,  1.88s/it, loss=0.298, acc=88.96%, lr=12025-02-07 21:12:08,603 - INFO - Epoch 2, Batch 321: Loss = 0.298, Acc = 88.96%\n",
      "2025-02-07 21:12:25,418 - INFO - Log Interval: Avg Loss = 0.0281, Avg Acc = 87.38%\n",
      "Epoch 2/3:  80%|▊| 330/415 [10:24<02:39,  1.88s/it, loss=0.554, acc=90.54%, lr=12025-02-07 21:12:27,222 - INFO - Epoch 2, Batch 331: Loss = 0.554, Acc = 90.54%\n",
      "2025-02-07 21:12:43,637 - INFO - Log Interval: Avg Loss = 0.0306, Avg Acc = 85.88%\n",
      "Epoch 2/3:  82%|▊| 340/415 [10:42<02:19,  1.86s/it, loss=0.720, acc=80.95%, lr=12025-02-07 21:12:45,377 - INFO - Epoch 2, Batch 341: Loss = 0.720, Acc = 80.95%\n",
      "2025-02-07 21:13:01,879 - INFO - Log Interval: Avg Loss = 0.0274, Avg Acc = 87.49%\n",
      "Epoch 2/3:  84%|▊| 350/415 [11:01<02:00,  1.85s/it, loss=0.435, acc=85.62%, lr=12025-02-07 21:13:03,711 - INFO - Epoch 2, Batch 351: Loss = 0.435, Acc = 85.62%\n",
      "2025-02-07 21:13:20,859 - INFO - Log Interval: Avg Loss = 0.0290, Avg Acc = 85.95%\n",
      "Epoch 2/3:  87%|▊| 360/415 [11:20<01:42,  1.86s/it, loss=0.452, acc=85.03%, lr=12025-02-07 21:13:22,647 - INFO - Epoch 2, Batch 361: Loss = 0.452, Acc = 85.03%\n",
      "2025-02-07 21:13:39,676 - INFO - Log Interval: Avg Loss = 0.0308, Avg Acc = 85.58%\n",
      "Epoch 2/3:  89%|▉| 370/415 [11:38<01:24,  1.87s/it, loss=0.428, acc=85.39%, lr=12025-02-07 21:13:41,608 - INFO - Epoch 2, Batch 371: Loss = 0.428, Acc = 85.39%\n",
      "2025-02-07 21:13:58,524 - INFO - Log Interval: Avg Loss = 0.0306, Avg Acc = 86.20%\n",
      "Epoch 2/3:  92%|▉| 380/415 [11:58<01:05,  1.88s/it, loss=0.338, acc=90.20%, lr=12025-02-07 21:14:00,670 - INFO - Epoch 2, Batch 381: Loss = 0.338, Acc = 90.20%\n",
      "2025-02-07 21:14:17,541 - INFO - Log Interval: Avg Loss = 0.0291, Avg Acc = 86.85%\n",
      "Epoch 2/3:  94%|▉| 390/415 [12:16<00:46,  1.88s/it, loss=0.575, acc=81.07%, lr=12025-02-07 21:14:19,352 - INFO - Epoch 2, Batch 391: Loss = 0.575, Acc = 81.07%\n",
      "2025-02-07 21:14:36,419 - INFO - Log Interval: Avg Loss = 0.0316, Avg Acc = 85.46%\n",
      "Epoch 2/3:  96%|▉| 400/415 [12:35<00:28,  1.89s/it, loss=0.261, acc=91.39%, lr=12025-02-07 21:14:38,429 - INFO - Epoch 2, Batch 401: Loss = 0.261, Acc = 91.39%\n",
      "2025-02-07 21:14:55,417 - INFO - Log Interval: Avg Loss = 0.0287, Avg Acc = 86.92%\n",
      "Epoch 2/3:  99%|▉| 410/415 [12:54<00:09,  1.88s/it, loss=0.357, acc=88.97%, lr=12025-02-07 21:14:57,148 - INFO - Epoch 2, Batch 411: Loss = 0.357, Acc = 88.97%\n",
      "Epoch 2/3: 100%|▉| 414/415 [13:01<00:01,  1.88s/it, loss=0.470, acc=87.04%, lr=12025-02-07 21:15:04,527 - INFO - Epoch 2, Batch 415: Loss = 0.470, Acc = 87.04%\n",
      "Epoch 2/3: 100%|▉| 414/415 [13:01<00:01,  1.89s/it, loss=0.470, acc=87.04%, lr=1\n",
      "2025-02-07 21:15:04,587 - INFO - Epoch 2 completed.\n",
      "2025-02-07 21:15:47,875 - INFO - Validation Results for Epoch 2: Loss = 1.0187, Accuracy = 75.60%\n",
      "\n",
      "Validation Results for Epoch 2:\n",
      "Loss: 1.0187\n",
      "Accuracy: 75.60%\n",
      "Epoch 3/3:   0%|      | 0/415 [00:00<?, ?it/s, acc=0.00%, loss=init, lr=1.0e-05]2025-02-07 21:15:47,936 - INFO - Starting Epoch 3/3\n",
      "Epoch 3/3:   0%|    | 0/415 [00:02<?, ?it/s, loss=0.236, acc=92.67%, lr=1.0e-05]2025-02-07 21:15:50,223 - INFO - Epoch 3, Batch 1: Loss = 0.236, Acc = 92.67%\n",
      "2025-02-07 21:16:08,862 - INFO - Log Interval: Avg Loss = 0.0157, Avg Acc = 92.83%\n",
      "Epoch 3/3:   2%| | 10/415 [00:22<13:46,  2.04s/it, loss=0.295, acc=89.60%, lr=1.2025-02-07 21:16:10,634 - INFO - Epoch 3, Batch 11: Loss = 0.295, Acc = 89.60%\n",
      "2025-02-07 21:16:27,788 - INFO - Log Interval: Avg Loss = 0.0159, Avg Acc = 92.67%\n",
      "Epoch 3/3:   5%| | 20/415 [00:41<12:54,  1.96s/it, loss=0.272, acc=90.97%, lr=1.2025-02-07 21:16:29,695 - INFO - Epoch 3, Batch 21: Loss = 0.272, Acc = 90.97%\n",
      "2025-02-07 21:16:47,781 - INFO - Log Interval: Avg Loss = 0.0165, Avg Acc = 92.41%\n",
      "Epoch 3/3:   7%| | 30/415 [01:01<12:41,  1.98s/it, loss=0.180, acc=94.71%, lr=1.2025-02-07 21:16:49,678 - INFO - Epoch 3, Batch 31: Loss = 0.180, Acc = 94.71%\n",
      "2025-02-07 21:17:06,700 - INFO - Log Interval: Avg Loss = 0.0142, Avg Acc = 93.52%\n",
      "Epoch 3/3:  10%| | 40/415 [01:20<12:08,  1.94s/it, loss=0.195, acc=94.20%, lr=1.2025-02-07 21:17:08,531 - INFO - Epoch 3, Batch 41: Loss = 0.195, Acc = 94.20%\n",
      "2025-02-07 21:17:25,828 - INFO - Log Interval: Avg Loss = 0.0156, Avg Acc = 92.61%\n",
      "Epoch 3/3:  12%| | 50/415 [01:40<11:48,  1.94s/it, loss=0.165, acc=94.44%, lr=1.2025-02-07 21:17:27,958 - INFO - Epoch 3, Batch 51: Loss = 0.165, Acc = 94.44%\n",
      "2025-02-07 21:17:44,704 - INFO - Log Interval: Avg Loss = 0.0141, Avg Acc = 93.41%\n",
      "Epoch 3/3:  14%|▏| 60/415 [01:58<11:19,  1.91s/it, loss=0.155, acc=93.87%, lr=1.2025-02-07 21:17:46,518 - INFO - Epoch 3, Batch 61: Loss = 0.155, Acc = 93.87%\n",
      "2025-02-07 21:18:03,106 - INFO - Log Interval: Avg Loss = 0.0165, Avg Acc = 92.65%\n",
      "Epoch 3/3:  17%|▏| 70/415 [02:16<10:51,  1.89s/it, loss=0.232, acc=93.23%, lr=1.2025-02-07 21:18:04,931 - INFO - Epoch 3, Batch 71: Loss = 0.232, Acc = 93.23%\n",
      "2025-02-07 21:18:21,255 - INFO - Log Interval: Avg Loss = 0.0141, Avg Acc = 93.22%\n",
      "Epoch 3/3:  19%|▏| 80/415 [02:35<10:24,  1.86s/it, loss=0.165, acc=96.12%, lr=1.2025-02-07 21:18:23,047 - INFO - Epoch 3, Batch 81: Loss = 0.165, Acc = 96.12%\n",
      "2025-02-07 21:18:39,403 - INFO - Log Interval: Avg Loss = 0.0146, Avg Acc = 93.47%\n",
      "Epoch 3/3:  22%|▏| 90/415 [02:53<10:00,  1.85s/it, loss=0.215, acc=93.90%, lr=1.2025-02-07 21:18:41,133 - INFO - Epoch 3, Batch 91: Loss = 0.215, Acc = 93.90%\n",
      "2025-02-07 21:18:57,686 - INFO - Log Interval: Avg Loss = 0.0142, Avg Acc = 93.46%\n",
      "Epoch 3/3:  24%|▏| 100/415 [03:11<09:40,  1.84s/it, loss=0.125, acc=93.91%, lr=12025-02-07 21:18:59,517 - INFO - Epoch 3, Batch 101: Loss = 0.125, Acc = 93.91%\n",
      "2025-02-07 21:19:16,063 - INFO - Log Interval: Avg Loss = 0.0140, Avg Acc = 93.32%\n",
      "Epoch 3/3:  27%|▎| 110/415 [03:30<09:22,  1.85s/it, loss=0.576, acc=84.48%, lr=12025-02-07 21:19:17,981 - INFO - Epoch 3, Batch 111: Loss = 0.576, Acc = 84.48%\n",
      "2025-02-07 21:19:34,344 - INFO - Log Interval: Avg Loss = 0.0145, Avg Acc = 93.18%\n",
      "Epoch 3/3:  29%|▎| 120/415 [03:48<09:01,  1.83s/it, loss=0.202, acc=95.09%, lr=12025-02-07 21:19:36,081 - INFO - Epoch 3, Batch 121: Loss = 0.202, Acc = 95.09%\n",
      "2025-02-07 21:19:53,243 - INFO - Log Interval: Avg Loss = 0.0146, Avg Acc = 93.37%\n",
      "Epoch 3/3:  31%|▎| 130/415 [04:07<08:48,  1.85s/it, loss=0.079, acc=97.59%, lr=12025-02-07 21:19:55,066 - INFO - Epoch 3, Batch 131: Loss = 0.079, Acc = 97.59%\n",
      "2025-02-07 21:20:11,496 - INFO - Log Interval: Avg Loss = 0.0161, Avg Acc = 91.90%\n",
      "Epoch 3/3:  34%|▎| 140/415 [04:25<08:26,  1.84s/it, loss=0.284, acc=92.99%, lr=12025-02-07 21:20:13,224 - INFO - Epoch 3, Batch 141: Loss = 0.284, Acc = 92.99%\n",
      "2025-02-07 21:20:29,645 - INFO - Log Interval: Avg Loss = 0.0156, Avg Acc = 92.97%\n",
      "Epoch 3/3:  36%|▎| 150/415 [04:43<08:08,  1.84s/it, loss=0.119, acc=96.97%, lr=12025-02-07 21:20:31,674 - INFO - Epoch 3, Batch 151: Loss = 0.119, Acc = 96.97%\n",
      "2025-02-07 21:20:48,190 - INFO - Log Interval: Avg Loss = 0.0142, Avg Acc = 93.40%\n",
      "Epoch 3/3:  39%|▍| 160/415 [05:01<07:48,  1.84s/it, loss=0.187, acc=93.10%, lr=12025-02-07 21:20:49,898 - INFO - Epoch 3, Batch 161: Loss = 0.187, Acc = 93.10%\n",
      "2025-02-07 21:21:06,903 - INFO - Log Interval: Avg Loss = 0.0134, Avg Acc = 93.58%\n",
      "Epoch 3/3:  41%|▍| 170/415 [05:20<07:32,  1.85s/it, loss=0.175, acc=95.00%, lr=12025-02-07 21:21:08,638 - INFO - Epoch 3, Batch 171: Loss = 0.175, Acc = 95.00%\n",
      "2025-02-07 21:21:25,084 - INFO - Log Interval: Avg Loss = 0.0148, Avg Acc = 93.01%\n",
      "Epoch 3/3:  43%|▍| 180/415 [05:39<07:14,  1.85s/it, loss=0.177, acc=92.19%, lr=12025-02-07 21:21:27,106 - INFO - Epoch 3, Batch 181: Loss = 0.177, Acc = 92.19%\n",
      "2025-02-07 21:21:43,825 - INFO - Log Interval: Avg Loss = 0.0140, Avg Acc = 93.33%\n",
      "Epoch 3/3:  46%|▍| 190/415 [05:57<06:56,  1.85s/it, loss=0.444, acc=86.28%, lr=12025-02-07 21:21:45,748 - INFO - Epoch 3, Batch 191: Loss = 0.444, Acc = 86.28%\n",
      "2025-02-07 21:22:02,449 - INFO - Log Interval: Avg Loss = 0.0152, Avg Acc = 92.56%\n",
      "Epoch 3/3:  48%|▍| 200/415 [06:16<06:40,  1.86s/it, loss=0.149, acc=93.67%, lr=12025-02-07 21:22:04,614 - INFO - Epoch 3, Batch 201: Loss = 0.149, Acc = 93.67%\n",
      "2025-02-07 21:22:21,227 - INFO - Log Interval: Avg Loss = 0.0150, Avg Acc = 92.60%\n",
      "Epoch 3/3:  51%|▌| 210/415 [06:35<06:21,  1.86s/it, loss=0.252, acc=92.55%, lr=12025-02-07 21:22:23,106 - INFO - Epoch 3, Batch 211: Loss = 0.252, Acc = 92.55%\n",
      "2025-02-07 21:22:40,055 - INFO - Log Interval: Avg Loss = 0.0141, Avg Acc = 93.14%\n",
      "Epoch 3/3:  53%|▌| 220/415 [06:53<06:03,  1.86s/it, loss=0.257, acc=90.48%, lr=12025-02-07 21:22:41,860 - INFO - Epoch 3, Batch 221: Loss = 0.257, Acc = 90.48%\n",
      "2025-02-07 21:22:58,722 - INFO - Log Interval: Avg Loss = 0.0130, Avg Acc = 93.34%\n",
      "Epoch 3/3:  55%|▌| 230/415 [07:12<05:44,  1.86s/it, loss=0.253, acc=93.04%, lr=12025-02-07 21:23:00,525 - INFO - Epoch 3, Batch 231: Loss = 0.253, Acc = 93.04%\n",
      "2025-02-07 21:23:17,714 - INFO - Log Interval: Avg Loss = 0.0142, Avg Acc = 93.32%\n",
      "Epoch 3/3:  58%|▌| 240/415 [07:31<05:27,  1.87s/it, loss=0.202, acc=92.54%, lr=12025-02-07 21:23:19,480 - INFO - Epoch 3, Batch 241: Loss = 0.202, Acc = 92.54%\n",
      "2025-02-07 21:23:36,359 - INFO - Log Interval: Avg Loss = 0.0143, Avg Acc = 93.49%\n",
      "Epoch 3/3:  60%|▌| 250/415 [07:50<05:10,  1.88s/it, loss=0.225, acc=92.54%, lr=12025-02-07 21:23:38,386 - INFO - Epoch 3, Batch 251: Loss = 0.225, Acc = 92.54%\n",
      "2025-02-07 21:23:55,390 - INFO - Log Interval: Avg Loss = 0.0140, Avg Acc = 93.28%\n",
      "Epoch 3/3:  63%|▋| 260/415 [08:09<04:50,  1.88s/it, loss=0.465, acc=87.50%, lr=12025-02-07 21:23:57,123 - INFO - Epoch 3, Batch 261: Loss = 0.465, Acc = 87.50%\n",
      "2025-02-07 21:24:13,029 - INFO - Log Interval: Avg Loss = 0.0149, Avg Acc = 93.27%\n",
      "Epoch 3/3:  65%|▋| 270/415 [08:26<04:27,  1.85s/it, loss=0.234, acc=94.06%, lr=12025-02-07 21:24:14,854 - INFO - Epoch 3, Batch 271: Loss = 0.234, Acc = 94.06%\n",
      "2025-02-07 21:24:31,278 - INFO - Log Interval: Avg Loss = 0.0154, Avg Acc = 92.36%\n",
      "Epoch 3/3:  67%|▋| 280/415 [08:45<04:08,  1.84s/it, loss=0.235, acc=92.92%, lr=12025-02-07 21:24:33,107 - INFO - Epoch 3, Batch 281: Loss = 0.235, Acc = 92.92%\n",
      "2025-02-07 21:24:50,010 - INFO - Log Interval: Avg Loss = 0.0147, Avg Acc = 93.18%\n",
      "Epoch 3/3:  70%|▋| 290/415 [09:03<03:50,  1.85s/it, loss=0.320, acc=95.69%, lr=12025-02-07 21:24:51,726 - INFO - Epoch 3, Batch 291: Loss = 0.320, Acc = 95.69%\n",
      "2025-02-07 21:25:08,151 - INFO - Log Interval: Avg Loss = 0.0142, Avg Acc = 93.45%\n",
      "Epoch 3/3:  72%|▋| 300/415 [09:22<03:31,  1.84s/it, loss=0.169, acc=92.77%, lr=12025-02-07 21:25:09,975 - INFO - Epoch 3, Batch 301: Loss = 0.169, Acc = 92.77%\n",
      "2025-02-07 21:25:26,735 - INFO - Log Interval: Avg Loss = 0.0154, Avg Acc = 92.52%\n",
      "Epoch 3/3:  75%|▋| 310/415 [09:40<03:13,  1.85s/it, loss=0.373, acc=92.00%, lr=12025-02-07 21:25:28,621 - INFO - Epoch 3, Batch 311: Loss = 0.373, Acc = 92.00%\n",
      "2025-02-07 21:25:46,027 - INFO - Log Interval: Avg Loss = 0.0152, Avg Acc = 92.62%\n",
      "Epoch 3/3:  77%|▊| 320/415 [09:59<02:57,  1.87s/it, loss=0.165, acc=94.43%, lr=12025-02-07 21:25:47,898 - INFO - Epoch 3, Batch 321: Loss = 0.165, Acc = 94.43%\n",
      "2025-02-07 21:26:04,992 - INFO - Log Interval: Avg Loss = 0.0150, Avg Acc = 92.74%\n",
      "Epoch 3/3:  80%|▊| 330/415 [10:18<02:39,  1.88s/it, loss=0.144, acc=95.45%, lr=12025-02-07 21:26:06,893 - INFO - Epoch 3, Batch 331: Loss = 0.144, Acc = 95.45%\n",
      "2025-02-07 21:26:23,704 - INFO - Log Interval: Avg Loss = 0.0132, Avg Acc = 93.53%\n",
      "Epoch 3/3:  82%|▊| 340/415 [10:37<02:20,  1.88s/it, loss=0.196, acc=94.15%, lr=12025-02-07 21:26:25,600 - INFO - Epoch 3, Batch 341: Loss = 0.196, Acc = 94.15%\n",
      "2025-02-07 21:26:42,699 - INFO - Log Interval: Avg Loss = 0.0140, Avg Acc = 93.33%\n",
      "Epoch 3/3:  84%|▊| 350/415 [10:56<02:02,  1.89s/it, loss=0.164, acc=95.45%, lr=12025-02-07 21:26:44,714 - INFO - Epoch 3, Batch 351: Loss = 0.164, Acc = 95.45%\n",
      "2025-02-07 21:27:01,962 - INFO - Log Interval: Avg Loss = 0.0144, Avg Acc = 93.02%\n",
      "Epoch 3/3:  87%|▊| 360/415 [11:15<01:44,  1.89s/it, loss=0.204, acc=93.23%, lr=12025-02-07 21:27:03,761 - INFO - Epoch 3, Batch 361: Loss = 0.204, Acc = 93.23%\n",
      "2025-02-07 21:27:19,985 - INFO - Log Interval: Avg Loss = 0.0133, Avg Acc = 93.72%\n",
      "Epoch 3/3:  89%|▉| 370/415 [11:34<01:24,  1.87s/it, loss=0.465, acc=87.84%, lr=12025-02-07 21:27:22,009 - INFO - Epoch 3, Batch 371: Loss = 0.465, Acc = 87.84%\n",
      "2025-02-07 21:27:38,416 - INFO - Log Interval: Avg Loss = 0.0152, Avg Acc = 93.15%\n",
      "Epoch 3/3:  92%|▉| 380/415 [11:52<01:05,  1.86s/it, loss=0.067, acc=98.35%, lr=12025-02-07 21:27:40,249 - INFO - Epoch 3, Batch 381: Loss = 0.067, Acc = 98.35%\n",
      "2025-02-07 21:27:57,102 - INFO - Log Interval: Avg Loss = 0.0146, Avg Acc = 93.36%\n",
      "Epoch 3/3:  94%|▉| 390/415 [12:11<00:46,  1.86s/it, loss=0.276, acc=90.65%, lr=12025-02-07 21:27:58,982 - INFO - Epoch 3, Batch 391: Loss = 0.276, Acc = 90.65%\n",
      "2025-02-07 21:28:15,150 - INFO - Log Interval: Avg Loss = 0.0142, Avg Acc = 93.29%\n",
      "Epoch 3/3:  96%|▉| 400/415 [12:29<00:27,  1.85s/it, loss=0.153, acc=95.64%, lr=12025-02-07 21:28:17,150 - INFO - Epoch 3, Batch 401: Loss = 0.153, Acc = 95.64%\n",
      "2025-02-07 21:28:33,716 - INFO - Log Interval: Avg Loss = 0.0130, Avg Acc = 93.77%\n",
      "Epoch 3/3:  99%|▉| 410/415 [12:47<00:09,  1.85s/it, loss=0.235, acc=92.11%, lr=12025-02-07 21:28:35,519 - INFO - Epoch 3, Batch 411: Loss = 0.235, Acc = 92.11%\n",
      "Epoch 3/3: 100%|▉| 414/415 [12:55<00:01,  1.86s/it, loss=0.323, acc=91.18%, lr=12025-02-07 21:28:43,254 - INFO - Epoch 3, Batch 415: Loss = 0.323, Acc = 91.18%\n",
      "Epoch 3/3: 100%|▉| 414/415 [12:55<00:01,  1.87s/it, loss=0.323, acc=91.18%, lr=1\n",
      "2025-02-07 21:28:43,336 - INFO - Epoch 3 completed.\n",
      "2025-02-07 21:29:26,592 - INFO - Validation Results for Epoch 3: Loss = 1.1283, Accuracy = 75.30%\n",
      "\n",
      "Validation Results for Epoch 3:\n",
      "Loss: 1.1283\n",
      "Accuracy: 75.30%\n",
      "/opt/software/python/envs/google_colab_gpu_2024/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/opt/software/python/envs/google_colab_gpu_2024/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/opt/software/python/envs/google_colab_gpu_2024/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/opt/software/python/envs/google_colab_gpu_2024/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "Full model saved for inference in window_1/epoch_3\n",
      "Full model saved for inference in window_1/epoch_3\n",
      "Full model saved for inference in window_1/epoch_3\n",
      "2025-02-07 21:29:48,839 - INFO - Saving full model (pytorch_model.bin) via torch.save\n",
      "Попытка сохранения через torch\n",
      "Торч успешен, сохранение через инференс\n",
      "2025-02-07 21:30:37,643 - INFO - Saving model for inference via inference_model.save_pretrained\n",
      "Loading checkpoint shards: 100%|████████████████| 10/10 [00:09<00:00,  1.10it/s]\n",
      "2025-02-07 21:32:34,602 - INFO - Full model saved for inference in window_1/epoch_3\n",
      "Full model saved for inference in window_1/epoch_3\n",
      "2025-02-07 21:36:39,229 - INFO - Final Test Results: Loss = 1.1682, Accuracy = 74.43%\n",
      "\n",
      "Final Test Results:\n",
      "Loss: 1.1682\n",
      "Accuracy: 74.43%\n"
     ]
    }
   ],
   "source": [
    "# nproc_per_node - количество GPU на которых будет запущен скрипт\n",
    "\n",
    "!torchrun --nproc_per_node=4 4_Mistral_QA_train_3E.py \\\n",
    "  --batch_size 1 \\\n",
    "  --gradient_accumulation 8 \\\n",
    "  --epochs 3 \\\n",
    "  --train_path window1_train.tsv \\\n",
    "  --valid_path window1_valid.tsv \\\n",
    "  --test_path window_all_test.tsv \\\n",
    "  --save_dir window_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
