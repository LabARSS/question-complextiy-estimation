{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "import error: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5689db3e497b40888692ecf31b558c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-4\"\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "if torch.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=DEVICE,\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.319014656\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint() / 10**9)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "LLMModel = Any\n",
    "\n",
    "\n",
    "# TODO: Cite https://github.com/abazarova/tda4hallucinations/\n",
    "@dataclass\n",
    "class TokenwiseEntropy:\n",
    "    llm_model: LLMModel\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calculate(self, input_ids, n) -> float:\n",
    "        token_distribution = self._get_token_distribution(input_ids, n)\n",
    "        entropy = self._compute_entropy_from_logits(token_distribution) / n\n",
    "        return entropy.detach().cpu().item()\n",
    "\n",
    "    def _get_token_distribution(self, input_ids, n) -> torch.Tensor:\n",
    "        # Yield the output of the model for the current example\n",
    "        output = self.llm_model(\n",
    "            input_ids,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=False,\n",
    "        )\n",
    "\n",
    "        return output.logits[0, -n:]\n",
    "\n",
    "    def _compute_entropy_from_logits(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute entropy from logits.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        logits : torch.Tensor\n",
    "            Logits from the model.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Entropy values.\n",
    "        \"\"\"\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        log_probabilities = torch.log(probabilities + 1e-12)\n",
    "        entropies = -torch.sum(probabilities * log_probabilities, dim=-1)\n",
    "        # print(entropies)\n",
    "        return entropies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def get_sys_prompt(subject: str | None = None):\n",
    "    if subject is not None:\n",
    "        sys_msg = f\"The following is a multiple choice question about {subject}.\"\n",
    "    else:\n",
    "        sys_msg = \"The following is a multiple choice question.\"\n",
    "\n",
    "    sys_msg += 'Please act as an expert and answer the question. Begin your answer by providing a short explanation. After providing your explanation, you must write down the NUMBER of the correct answer by strictly following the format: \"[[answer]]\".'\n",
    "    return sys_msg\n",
    "\n",
    "\n",
    "option_ids = [str(i + 1) for i in range(20)]\n",
    "\n",
    "\n",
    "def get_user_prompt(question: str, options: List[str]):\n",
    "    options_str = \"\\n\".join([f\"{option_id}. {answer}\".strip() for option_id, answer in zip(option_ids, options)])\n",
    "    user_prompt = f'Question: {question.strip()}\\nOptions:\\n{options_str}\\nProvide a short explanation and choose one of the answers. After that, write down the NUMBER of the correct answer in the format: \"[[answer]]\"'\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Karl Llewellyn, a prominent legal realist, distinguished between the \"grand style\" and \"formal style\" of legal reasoning. The grand style involves a more creative and policy-oriented approach, where judges may consider broader social implications and policy goals. In contrast, the formal style is more rigid, focusing strictly on applying established legal rules and precedents.\n",
      "\n",
      "Criticism of Llewellyn's distinction often centers on the practical application of these styles in judicial decision-making. One compelling criticism is that it is misleading to pigeon-hole judges into these distinct categories. In reality, judicial reasoning often involves a blend of both styles. Judges may start with a formal analysis but then shift to a grand style when the formal rules are indeterminate or lead to unjust outcomes. This criticism highlights the fluidity and complexity of judicial reasoning, which cannot be neatly categorized.\n",
      "\n",
      "Therefore, the most compelling criticism is that it is misleading to pigeon-hole judges in this way.\n",
      "\n",
      "[[3]]\n",
      "tensor([1.0498e-01, 3.2031e-01, 9.3079e-04, 4.6539e-04, 6.1798e-04, 1.0156e+00,\n",
      "        2.2168e-01, 1.1875e+00, 6.9922e-01, 2.5195e-01, 4.2021e-06, 8.6914e-02,\n",
      "        1.4531e+00, 6.6833e-03, 1.4062e+00, 5.3906e-01, 1.9141e-01, 2.1094e-01,\n",
      "        1.4343e-03, 3.4375e-01, 6.8750e-01, 4.9133e-03, 1.6689e-04, 5.6396e-02,\n",
      "        7.3433e-05, 3.5645e-02, 1.7676e-01, 3.0060e-03, 4.1797e-01, 5.2490e-02,\n",
      "        7.7344e-01, 6.6280e-05, 1.4375e+00, 2.2969e+00, 8.6328e-01, 2.5000e+00,\n",
      "        8.9062e-01, 1.7344e+00, 7.3828e-01, 1.4746e-01, 7.5391e-01, 2.0469e+00,\n",
      "        5.6152e-02, 2.5781e+00, 2.6250e+00, 4.6875e-01, 8.3984e-01, 1.6016e+00,\n",
      "        3.4961e-01, 3.5625e+00, 1.4922e+00, 1.1797e+00, 8.5547e-01, 4.6921e-04,\n",
      "        3.7104e-06, 6.0425e-03, 8.6784e-05, 1.6332e-05, 1.5547e+00, 8.5547e-01,\n",
      "        2.0938e+00, 7.5000e-01, 1.2266e+00, 6.8359e-01, 2.2125e-04, 2.0938e+00,\n",
      "        1.3516e+00, 8.3594e-01, 4.4922e-01, 3.4570e-01, 2.4805e-01, 8.9645e-04,\n",
      "        1.1953e+00, 1.1172e+00, 6.9141e-01, 5.3125e-01, 7.1484e-01, 9.7752e-05,\n",
      "        8.4686e-04, 9.5749e-04, 3.1445e-01, 1.4258e-01, 9.5703e-01, 1.1094e+00,\n",
      "        4.2188e-01, 1.0938e+00, 1.9453e+00, 2.4062e+00, 8.0469e-01, 3.4375e-01,\n",
      "        1.8359e-01, 1.1016e+00, 6.4453e-01, 7.3047e-01, 4.5586e-04, 1.2598e-01,\n",
      "        1.7031e+00, 1.5391e+00, 3.0273e-01, 6.7383e-02, 1.7969e-01, 1.5547e+00,\n",
      "        1.1094e+00, 1.1797e+00, 3.1738e-03, 4.2773e-01, 1.4551e-01, 3.6240e-04,\n",
      "        8.9844e-01, 1.5781e+00, 1.4609e+00, 4.4531e-01, 1.0938e+00, 1.2969e+00,\n",
      "        5.1953e-01, 1.9312e-05, 7.6953e-01, 1.8652e-01, 7.8516e-01, 6.0938e-01,\n",
      "        1.6309e-01, 5.5469e-01, 8.0566e-03, 1.2512e-02, 2.6562e-01, 7.6562e-01,\n",
      "        7.4158e-03, 6.0156e-01, 8.3984e-01, 5.7617e-02, 4.1211e-01, 1.3611e-02,\n",
      "        1.3750e+00, 1.3125e+00, 1.9609e+00, 2.2812e+00, 4.4531e-01, 2.0996e-01,\n",
      "        4.2383e-01, 3.5352e-01, 1.2344e+00, 2.4062e+00, 1.7422e+00, 1.3906e+00,\n",
      "        9.3750e-01, 1.4766e+00, 1.8311e-03, 5.0537e-02, 2.0625e+00, 6.5267e-06,\n",
      "        8.1641e-01, 4.2578e-01, 5.3467e-02, 6.0156e-01, 2.0000e+00, 7.7344e-01,\n",
      "        3.0396e-02, 7.5781e-01, 8.0469e-01, 3.4424e-02, 8.9453e-01, 3.5156e-02,\n",
      "        2.0996e-01, 6.3281e-01, 5.5176e-02, 9.4141e-01, 6.8750e-01, 4.3945e-02,\n",
      "        3.4766e-01, 6.8359e-01, 7.4609e-01, 9.6875e-01, 7.9870e-06, 8.9722e-03,\n",
      "        9.5215e-03, 2.5511e-05, 1.1063e-03, 8.7402e-02, 7.3438e-01, 1.7188e-01,\n",
      "        1.4160e-02, 2.4872e-03, 1.0073e-05, 1.1047e-02, 5.2261e-04, 4.5868e-08,\n",
      "        6.1340e-03, 1.2665e-03, 1.6689e-04, 7.0312e-01, 5.3955e-02, 4.9072e-02,\n",
      "        1.4782e-04, 9.0027e-04, 1.7390e-09], device='mps:0',\n",
      "       dtype=torch.bfloat16)\n",
      "Answer: Karl Llewellyn, a prominent legal realist, distinguished between the \"grand style\" and \"formal style\" of legal reasoning. The grand style involves a more creative and policy-oriented approach, where judges may consider broader social implications and policy goals. In contrast, the formal style is more rigid, focusing strictly on applying established legal rules and precedents.\n",
      "\n",
      "Criticism of Llewellyn's distinction often centers on the practical application of these styles in judicial decision-making. One compelling criticism is that it is misleading to pigeon-hole judges into these distinct categories. In reality, judicial reasoning often involves a blend of both styles. Judges may start with a formal analysis but then shift to a grand style when the formal rules are indeterminate or lead to unjust outcomes. This criticism highlights the fluidity and complexity of judicial reasoning, which cannot be neatly categorized.\n",
      "\n",
      "Therefore, the most compelling criticism is that it is misleading to pigeon-hole judges in this way.\n",
      "\n",
      "[[3]]\n",
      "Entropy: 0.000537872314453125\n",
      "is_correct: True\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/12032 [01:16<255:10:16, 76.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: In the context of the Indian Constitution, \"qualified rights\" are those rights that are not absolute and can be restricted under certain conditions. Article 19, which deals with the freedom of speech and expression, assembly, association, movement, residence, and profession, is a qualified right because these freedoms can be restricted on grounds such as the sovereignty and integrity of India, security of the state, public order, decency, morality, etc.\n",
      "\n",
      "On the other hand, \"unqualified rights\" or \"absolute rights\" are those that cannot be restricted or are not subject to any conditions. Article 21, which guarantees the right to life and personal liberty, is often considered an unqualified right, although it has been subject to reasonable restrictions through judicial interpretation.\n",
      "\n",
      "Article 12 defines the term \"State\" for the purposes of Part III of the Constitution, which deals with Fundamental Rights. It is not a right itself but a definitional provision.\n",
      "\n",
      "Article 11 deals with the power of Parliament to regulate the right of citizenship by law.\n",
      "\n",
      "Article 9 prohibits the conferment of titles, except military and academic distinctions.\n",
      "\n",
      "Article 3 deals with the formation of new states and alteration of areas, boundaries, or names of existing states.\n",
      "\n",
      "Article 17 abolishes \"untouchability\" and forbids its practice in any form.\n",
      "\n",
      "Among the options provided, Article 12 is not a right but a definitional article, and thus it is not a qualified right.\n",
      "\n",
      "[[2]]\n",
      "tensor([8.3203e-01, 2.1191e-01, 2.2054e-05, 1.1797e+00, 2.5781e-01, 1.1873e-04,\n",
      "        1.5747e-02, 1.7344e+00, 2.8516e-01, 3.0151e-02, 1.8463e-03, 6.6406e-01,\n",
      "        4.5117e-01, 1.1484e+00, 4.1797e-01, 1.1484e+00, 6.7969e-01, 5.9204e-03,\n",
      "        9.2773e-02, 3.9844e-01, 1.6327e-03, 1.2266e+00, 7.3047e-01, 3.2812e-01,\n",
      "        6.5234e-01, 2.1250e+00, 1.9062e+00, 1.9312e-05, 4.8218e-03, 1.0391e+00,\n",
      "        4.3555e-01, 1.6562e+00, 6.1798e-04, 8.5938e-01, 1.3125e+00, 7.8964e-04,\n",
      "        4.6539e-04, 8.1543e-02, 9.3384e-03, 1.4844e-01, 3.2422e-01, 1.6594e-04,\n",
      "        3.9307e-02, 8.8379e-02, 7.9346e-03, 8.9645e-04, 2.2583e-02, 3.2043e-03,\n",
      "        1.9897e-02, 2.1094e-01, 3.4668e-02, 1.7383e-01, 1.1953e+00, 1.2500e+00,\n",
      "        8.3542e-04, 9.1406e-01, 1.0703e+00, 1.9434e-01, 3.6523e-01, 1.9455e-03,\n",
      "        1.3516e+00, 1.2812e+00, 1.7734e+00, 1.1172e+00, 1.2517e-05, 1.3281e+00,\n",
      "        5.4297e-01, 6.2500e-02, 1.9684e-03, 5.2691e-05, 3.2715e-02, 3.6774e-03,\n",
      "        8.9844e-01, 4.6692e-03, 1.8616e-03, 1.1035e-01, 1.7452e-04, 2.4609e-01,\n",
      "        5.4169e-04, 8.2493e-05, 6.8359e-02, 3.2227e-01, 3.8477e-01, 3.3951e-04,\n",
      "        1.2656e+00, 3.0273e-01, 1.6172e+00, 1.0777e-04, 4.8218e-03, 5.5695e-04,\n",
      "        1.8463e-03, 1.4062e+00, 4.3359e-01, 7.5195e-02, 2.5977e-01, 1.6211e-01,\n",
      "        9.3750e-01, 2.3047e-01, 2.0215e-01, 9.3079e-04, 7.2937e-03, 8.0469e-01,\n",
      "        4.6680e-01, 8.9453e-01, 8.8672e-01, 3.9368e-03, 1.5859e+00, 2.0781e+00,\n",
      "        2.3750e+00, 1.3438e+00, 2.7539e-01, 4.3678e-04, 7.1484e-01, 1.9531e+00,\n",
      "        8.8281e-01, 1.6562e+00, 2.4605e-04, 1.8750e+00, 4.1797e-01, 6.8359e-01,\n",
      "        1.4531e+00, 7.7148e-02, 6.5234e-01, 9.9182e-05, 7.9346e-03, 1.5354e-04,\n",
      "        9.9487e-03, 8.6308e-05, 1.4832e-02, 5.7422e-01, 1.4141e+00, 1.1016e+00,\n",
      "        7.3047e-01, 5.9766e-01, 8.8692e-05, 6.3477e-02, 1.3594e+00, 1.4219e+00,\n",
      "        1.9453e+00, 1.4141e+00, 1.1406e+00, 1.2031e+00, 4.5204e-04, 1.9375e+00,\n",
      "        1.3367e-02, 2.0000e+00, 1.0938e+00, 4.7656e-01, 5.5469e-01, 2.2656e+00,\n",
      "        3.0899e-04, 1.2500e+00, 1.7031e+00, 8.3496e-02, 1.8945e-01, 4.1504e-02,\n",
      "        4.1748e-02, 1.3770e-01, 2.6367e-01, 1.1719e-01, 2.3633e-01, 4.4823e-04,\n",
      "        1.1035e-01, 3.6163e-03, 6.2500e-01, 3.1710e-05, 5.7617e-02, 9.1797e-01,\n",
      "        3.6133e-01, 5.1562e-01, 5.6028e-05, 1.5918e-01, 1.0300e-04, 3.8867e-01,\n",
      "        2.6953e-01, 6.2109e-01, 5.0781e-01, 1.3184e-01, 5.1953e-01, 1.1172e+00,\n",
      "        2.9688e-01, 6.6406e-01, 6.2500e-01, 5.0783e-05, 8.7500e-01, 1.7500e+00,\n",
      "        1.1484e+00, 2.3842e-05, 6.0938e-01, 1.9688e+00, 7.6294e-05, 2.5195e-01,\n",
      "        1.1328e+00, 2.5635e-03, 2.0117e-01, 1.5869e-03, 5.7422e-01, 9.3750e-01,\n",
      "        9.5703e-01, 4.2383e-01, 5.8838e-02, 1.7383e-01, 2.0386e-02, 1.0703e+00,\n",
      "        4.9744e-03, 1.0490e-05, 1.6797e-01, 1.4766e+00, 2.2969e+00, 2.0469e+00,\n",
      "        5.6982e-05, 9.3079e-04, 3.4766e-01, 1.2656e+00, 5.3125e-01, 1.2402e-01,\n",
      "        4.2773e-01, 1.8188e-02, 2.6172e-01, 6.4453e-01, 1.8158e-03, 1.6093e-05,\n",
      "        8.2031e-02, 1.0000e+00, 2.2650e-06, 1.2970e-03, 4.7266e-01, 1.0132e-02,\n",
      "        6.4087e-04, 1.7578e-01, 1.3245e-02, 5.3955e-02, 1.6499e-04, 2.1606e-02,\n",
      "        1.3733e-02, 8.7280e-03, 4.0039e-02, 1.5320e-02, 3.2234e-04, 3.1090e-04,\n",
      "        1.5488e-03, 1.2589e-04, 9.8828e-01, 1.7242e-03, 4.5076e-07, 2.0027e-05,\n",
      "        8.5449e-02, 1.7762e-05, 9.6191e-02, 1.9434e-01, 1.7762e-05, 1.6308e-04,\n",
      "        3.6377e-02, 1.1396e-04, 9.3262e-02, 1.1015e-04, 3.2997e-04, 1.0834e-03,\n",
      "        3.6133e-02, 2.4199e-05, 2.6512e-04, 8.5547e-01, 7.7344e-01, 4.2188e-01,\n",
      "        2.6172e-01, 4.0234e-01, 5.4443e-02, 1.9043e-01, 1.7583e-06, 5.0537e-02,\n",
      "        2.8125e-01, 9.5215e-02, 1.1292e-02, 7.6562e-01, 7.2656e-01, 1.2268e-02,\n",
      "        2.1777e-01, 3.8743e-06, 1.0469e+00, 1.1562e+00, 5.7812e-01, 1.2031e+00,\n",
      "        9.9609e-01, 5.1172e-01, 3.8672e-01, 1.1250e+00, 1.8848e-01, 2.7148e-01,\n",
      "        7.2266e-01, 1.0859e+00, 1.8066e-01, 3.7384e-04, 1.3428e-03, 2.4011e-09],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "Answer: In the context of the Indian Constitution, \"qualified rights\" are those rights that are not absolute and can be restricted under certain conditions. Article 19, which deals with the freedom of speech and expression, assembly, association, movement, residence, and profession, is a qualified right because these freedoms can be restricted on grounds such as the sovereignty and integrity of India, security of the state, public order, decency, morality, etc.\n",
      "\n",
      "On the other hand, \"unqualified rights\" or \"absolute rights\" are those that cannot be restricted or are not subject to any conditions. Article 21, which guarantees the right to life and personal liberty, is often considered an unqualified right, although it has been subject to reasonable restrictions through judicial interpretation.\n",
      "\n",
      "Article 12 defines the term \"State\" for the purposes of Part III of the Constitution, which deals with Fundamental Rights. It is not a right itself but a definitional provision.\n",
      "\n",
      "Article 11 deals with the power of Parliament to regulate the right of citizenship by law.\n",
      "\n",
      "Article 9 prohibits the conferment of titles, except military and academic distinctions.\n",
      "\n",
      "Article 3 deals with the formation of new states and alteration of areas, boundaries, or names of existing states.\n",
      "\n",
      "Article 17 abolishes \"untouchability\" and forbids its practice in any form.\n",
      "\n",
      "Among the options provided, Article 12 is not a right but a definitional article, and thus it is not a qualified right.\n",
      "\n",
      "[[2]]\n",
      "Entropy: 0.0028228759765625\n",
      "is_correct: False\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/12032 [02:45<279:35:59, 83.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Ensuring that one individual does not carry the burden of a whole work task involves assigning parts of the task to different individuals or teams. This process helps in optimizing efficiency, utilizing diverse skills, and preventing burnout. The term that best describes this concept is \"work delegation.\" Work delegation involves assigning responsibility and authority to others to complete specific tasks or projects, thereby distributing the workload among team members.\n",
      "\n",
      "[[1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/12032 [03:25<213:53:25, 64.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1325e-05, 5.1880e-04, 2.1582e-01, 6.3782e-03, 1.6113e-02, 6.6161e-06,\n",
      "        1.0840e-01, 2.3365e-04, 1.5918e-01, 1.3447e-04, 5.5469e-01, 4.3945e-03,\n",
      "        3.0884e-02, 7.1526e-05, 1.1406e+00, 1.7812e+00, 1.7031e+00, 2.2656e-01,\n",
      "        8.3984e-01, 2.8125e-01, 2.5586e-01, 8.3594e-01, 6.6016e-01, 1.1094e+00,\n",
      "        5.5859e-01, 6.6797e-01, 2.2827e-02, 2.0156e+00, 1.4219e+00, 1.2734e+00,\n",
      "        2.2812e+00, 1.7031e+00, 6.4844e-01, 2.5781e+00, 1.9297e+00, 5.6250e-01,\n",
      "        2.8711e-01, 2.2754e-01, 1.1484e+00, 1.4219e+00, 3.9673e-03, 1.5938e+00,\n",
      "        1.8750e+00, 2.2031e+00, 9.4531e-01, 1.0938e+00, 8.0078e-01, 2.2070e-01,\n",
      "        1.8906e+00, 2.8906e-01, 1.8652e-01, 7.1094e-01, 6.3672e-01, 7.5391e-01,\n",
      "        8.5547e-01, 6.6280e-05, 1.1953e+00, 1.7109e+00, 1.1797e+00, 5.9766e-01,\n",
      "        7.0801e-02, 9.9219e-01, 1.6797e+00, 8.1250e-01, 1.0469e+00, 1.4688e+00,\n",
      "        2.0703e-01, 9.1406e-01, 2.3750e+00, 9.8047e-01, 2.1250e+00, 1.3750e+00,\n",
      "        2.1387e-01, 3.6328e-01, 1.8828e+00, 1.4531e+00, 2.3746e-04, 1.5156e+00,\n",
      "        1.9375e+00, 1.2390e-02, 2.8687e-03, 3.6316e-03, 3.5361e-09],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "Answer: Ensuring that one individual does not carry the burden of a whole work task involves assigning parts of the task to different individuals or teams. This process helps in optimizing efficiency, utilizing diverse skills, and preventing burnout. The term that best describes this concept is \"work delegation.\" Work delegation involves assigning responsibility and authority to others to complete specific tasks or projects, thereby distributing the workload among team members.\n",
      "\n",
      "[[1]]\n",
      "Entropy: 1.3690441846847534e-07\n",
      "is_correct: False\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/12032 [03:46<252:32:56, 75.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 113\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mestimate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_subject_from_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_cluster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_question_from_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_options_from_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_model_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 48\u001b[0m, in \u001b[0;36mestimate_dataset\u001b[0;34m(df, model, tokenizer, get_subject_from_row, get_question_from_row, get_options_from_row, verify_answer, out_filename)\u001b[0m\n\u001b[1;32m     44\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(formatted_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# print(f\"loop {index} -> after generate: {model.get_memory_footprint(return_buffers=True) / 10**9} GB\")\u001b[39;00m\n\u001b[1;32m     51\u001b[0m input_length \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/dev/sktech/phi-4/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/sktech/phi-4/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/sktech/phi-4/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3243\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3242\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3244\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3245\u001b[0m ):\n\u001b[1;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3249\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import csv\n",
    "import gc\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "DUMP_EVERY = 100\n",
    "invalid_answers = 0\n",
    "\n",
    "\n",
    "def estimate_dataset(\n",
    "    df, model, tokenizer, get_subject_from_row, get_question_from_row, get_options_from_row, verify_answer, out_filename\n",
    "):\n",
    "    global invalid_answers\n",
    "\n",
    "    model_name = model.config_class().model_type\n",
    "    print(model_name)\n",
    "\n",
    "    field_ans_correct = f\"entropy_cot_ans_correct_{model_name}\"\n",
    "    field_entropy_value = f\"entropy_cot_value_{model_name}\"\n",
    "\n",
    "    if field_ans_correct not in df.columns:\n",
    "        df[field_ans_correct] = False\n",
    "    if field_entropy_value not in df.columns:\n",
    "        df[field_entropy_value] = 0.0\n",
    "\n",
    "    entropy_estimator = TokenwiseEntropy(llm_model=model, device=DEVICE)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if df.at[index, field_entropy_value] != 0.0:\n",
    "            continue\n",
    "\n",
    "        # print(f\"loop {index} -> start: {model.get_memory_footprint(return_buffers=True) / 10**9} GB\")\n",
    "\n",
    "        sys_prompt = get_sys_prompt(get_subject_from_row(row))\n",
    "        user_prompt = get_user_prompt(get_question_from_row(row), get_options_from_row(row))\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id)\n",
    "        # print(f\"loop {index} -> after generate: {model.get_memory_footprint(return_buffers=True) / 10**9} GB\")\n",
    "\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        n = outputs.shape[1] - input_length\n",
    "        answer_raw = outputs[0, input_length:]\n",
    "        answer_str = tokenizer.decode(answer_raw, skip_special_tokens=True)\n",
    "        # print(f\"Answer: {answer_str}\")\n",
    "        try:\n",
    "            answer = re.search(\"\\\\[\\\\[(\\\\d+?)\\\\]\\\\]\", answer_str).group(1)\n",
    "            if answer in option_ids:\n",
    "                entropy = entropy_estimator.calculate(outputs, n)\n",
    "                # print(f\"loop {index} -> after entropy: {model.get_memory_footprint(return_buffers=True) / 10**9} GB\")\n",
    "                df.at[index, field_entropy_value] = entropy\n",
    "                df.at[index, field_ans_correct] = verify_answer(row, answer)\n",
    "            else:\n",
    "                invalid_answers += 1\n",
    "        except:\n",
    "            invalid_answers += 1\n",
    "\n",
    "        # print(\n",
    "        #     f\"Answer: {answer_str}\\nEntropy: {df.at[index, field_entropy_value]}\\nis_correct: {df.at[index, field_ans_correct]}\\n\\n\"\n",
    "        # )\n",
    "\n",
    "        if index % DUMP_EVERY == 0:\n",
    "            df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "\n",
    "        gc.collect()\n",
    "        if DEVICE == torch.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "    print(f\"Processed dataset {out_filename}. Total entries: {df.shape[0]}. Invalid answers: {invalid_answers}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "ORIGINAL_DATASET = \"../data/mmlu_pro_stem\"\n",
    "original_filename = f\"{ORIGINAL_DATASET}.tsv\"\n",
    "out_filename = f\"{ORIGINAL_DATASET}_w_phi4_entropy_cot.tsv\"\n",
    "\n",
    "if os.path.isfile(out_filename):\n",
    "    df = pd.read_csv(\n",
    "        out_filename,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        quotechar=\"\",\n",
    "        escapechar=\"\\\\\",\n",
    "    )\n",
    "else:\n",
    "    df = pd.read_csv(\n",
    "        original_filename,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "    )\n",
    "# df = df.head(10)\n",
    "\n",
    "\n",
    "def verify_model_answer(row, model_answer):\n",
    "    try:\n",
    "        return int(row[\"answer_index\"]) + 1 == int(model_answer)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "estimate_dataset(\n",
    "    df=df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    get_subject_from_row=lambda row: row[\"base_cluster\"],\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=lambda row: ast.literal_eval(row[\"options\"]),\n",
    "    verify_answer=verify_model_answer,\n",
    "    out_filename=out_filename,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
