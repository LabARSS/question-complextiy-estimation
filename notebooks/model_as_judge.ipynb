{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-large-2411 Determining the \"best\" French cheese can\n",
      "mistral-large-2411 Choosing the \"best\" French cheese can be\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_keys = os.environ[\"MISTRAL_API_KEYS\"]\n",
    "model = \"mistral-large-2411\"\n",
    "\n",
    "clients = [\n",
    "    Mistral(\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    for api_key in api_keys.split(\",\")\n",
    "]\n",
    "\n",
    "for client in clients:\n",
    "    # Check API is alive\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the best French cheese?\",\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    print(chat_response.model, chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "from mistralai import SDKError\n",
    "from time import sleep\n",
    "\n",
    "DEFAULT_SLEEP_DURATION = 1.5\n",
    "ADJUSTED_SLEEP_DURATION = DEFAULT_SLEEP_DURATION / len(clients)\n",
    "\n",
    "print(\"Sleep duration:\", ADJUSTED_SLEEP_DURATION)\n",
    "\n",
    "\n",
    "def wait(duration=ADJUSTED_SLEEP_DURATION):\n",
    "    sleep(duration)\n",
    "\n",
    "\n",
    "api_limit_hits = 0\n",
    "\n",
    "\n",
    "def repeat_if_hit_api_limit(f):  # (1)\n",
    "    def wrapper(*args, **kw):  # (2)\n",
    "        global api_limit_hits\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                return f(*args, **kw)\n",
    "            except RateLimitError:\n",
    "                wait()\n",
    "            except SDKError as e:\n",
    "                if e.status_code == 429:\n",
    "                    api_limit_hits += 1\n",
    "                    if (api_limit_hits % 10) == 0:\n",
    "                        print(f\"API limit hit {api_limit_hits} times\")\n",
    "                    wait()\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(\"repeat_if_hit_api_limit -> unknown error\", e)\n",
    "                wait(60)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_id = 0\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def query_model(messages):\n",
    "    global request_id\n",
    "    client = clients[request_id % len(clients)]\n",
    "    response = client.chat.complete(model=model, messages=messages)\n",
    "    request_id += 1\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.prompt' from '/Users/aigoncharov/dev/sktech/phi-4/utils/prompt.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "import utils.prompt as prompt\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Required to purge the module cache and use the latest version after an update\n",
    "importlib.reload(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3714/12032 [02:49<5:07:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract rating from response:\n",
      "The assistant's response provides a clear and concise explanation of the question's requirements and the underlying concepts being tested. The question involves understanding the order of operations and exponentiation in Python, which are fundamental concepts in mathematics and programming. The explanation correctly identifies that the student needs to know that exponentiation (** ) is evaluated before multiplication (*) and that any number raised to the power of 1 remains 1. This level of understanding is typically expected at the middle school level, making the complexity rating appropriate.\n",
      "\n",
      "Complexity: [[middle_school]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3720/12032 [03:36<16:24:44,  7.11s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "import utils.prompt as prompt\n",
    "\n",
    "difficulty = [\"middle_school\", \"high_school\", \"undergraduate\", \"postgraduate\", \"phd\"]\n",
    "ratings = list(range(1, 11, 1))\n",
    "\n",
    "invalid_complexities = 0\n",
    "invalid_ratings = 0\n",
    "\n",
    "\n",
    "def model_as_judge(index, system_prompt, user_prompt, answer):\n",
    "    global invalid_ratings\n",
    "\n",
    "    chat_response = query_model(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user request displayed below. Your evaluation should consider factors such as the following all the settings in the system prompt, correspondences to the context of the user, the helpfulness, relevance and accuracy. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example:\"Rating: [[6]]\".',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Instructions for Assistant]\n",
    "                {system_prompt}\n",
    "                [End of Instructions for Assistant]\n",
    "\n",
    "                [Question]\n",
    "                {user_prompt}\n",
    "                [End of Question]\n",
    "\n",
    "                [The Start of Assistant’s Answer]\n",
    "                {answer}\n",
    "                [The End of Assistant’s Answer]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        rating = re.search(\"\\\\[\\\\[(\\\\d+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(rating)\n",
    "        rating_int = int(rating)\n",
    "        if rating_int in ratings:\n",
    "            df.at[index, \"masj_rating\"] = rating_int\n",
    "        else:\n",
    "            invalid_ratings += 1\n",
    "    except:\n",
    "        print(f\"Could not extract rating from response:\\n{response}\\n\")\n",
    "        invalid_ratings += 1\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def estimate_complextiy_with_model(index, system_prompt, user_prompt):\n",
    "    global invalid_complexities\n",
    "\n",
    "    chat_response = query_model(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Question Start]\n",
    "                {user_prompt}\n",
    "                [Question End]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        complexity = re.search(\"\\\\[\\\\[(.+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(complexity)\n",
    "\n",
    "        if complexity in difficulty:\n",
    "            df.at[index, \"masj_complexity\"] = complexity\n",
    "        else:\n",
    "            invalid_complexities += 1\n",
    "    except:\n",
    "        print(f\"Could not extract complexity from response:\\n{response}\\n\")\n",
    "        invalid_complexities += 1\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "DUMP_EVERY = 100\n",
    "\n",
    "\n",
    "def estimate_dataset(df, get_question_from_row, get_options_from_row, out_filename):\n",
    "    if \"masj_complexity\" not in df.columns:\n",
    "        df[\"masj_complexity\"] = \"\"\n",
    "    if \"masj_rating\" not in df.columns:\n",
    "        df[\"masj_rating\"] = 0\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if df.at[index, \"masj_complexity\"] in difficulty and df.at[index, \"masj_rating\"] in ratings:\n",
    "            continue\n",
    "\n",
    "        complexity_system_prompt = f'You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity by strictly following the scale: {\", \".join(difficulty)}. You must return the complexity by strictly following this format: \"[[complexity]]\", for example: \"Complexity: [[middle_school]]\".'\n",
    "        complexity_user_prompt = prompt.get_user_prompt(get_question_from_row(row), get_options_from_row(row))\n",
    "\n",
    "        response_complexity = estimate_complextiy_with_model(index, complexity_system_prompt, complexity_user_prompt)\n",
    "        wait()\n",
    "\n",
    "        model_as_judge(index, complexity_system_prompt, complexity_user_prompt, response_complexity)\n",
    "        wait()\n",
    "\n",
    "        if index % DUMP_EVERY == 0:\n",
    "            df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "            print(f\"Over {DUMP_EVERY} iterations we hit {api_limit_hits} API limits\")\n",
    "\n",
    "    df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "    print(\n",
    "        f\"Processed dataset {out_filename}. Total entries: {df.shape[0]}. Invalid complexities: {invalid_complexities}. Invalid ratings: {invalid_ratings}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "ORIGINAL_DATASET = \"../data/mmlu_pro_stem\"\n",
    "original_filename = f\"{ORIGINAL_DATASET}.tsv\"\n",
    "out_filename = f\"{ORIGINAL_DATASET}_w_maj_complexity.tsv\"\n",
    "\n",
    "if os.path.isfile(out_filename):\n",
    "    df = pd.read_csv(\n",
    "        out_filename,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        quotechar=\"\",\n",
    "        escapechar=\"\\\\\",\n",
    "    )\n",
    "else:\n",
    "    df = pd.read_csv(\n",
    "        original_filename,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "    )\n",
    "# df = df.head(10)\n",
    "\n",
    "\n",
    "estimate_dataset(\n",
    "    df=df,\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=lambda row: ast.literal_eval(row[\"options\"]),\n",
    "    out_filename=out_filename,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
