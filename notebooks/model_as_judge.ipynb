{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-large-2411 Determining the \"best\" French cheese can\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-2411\"\n",
    "\n",
    "client = Mistral(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Check API is alive\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best French cheese?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=10,\n",
    ")\n",
    "print(chat_response.model, chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "from mistralai import SDKError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def wait(duration=1.5):\n",
    "    sleep(duration)\n",
    "\n",
    "\n",
    "def repeat_if_hit_api_limit(f):  # (1)\n",
    "    def wrapper(*args, **kw):  # (2)\n",
    "        while True:\n",
    "            try:\n",
    "                return f(*args, **kw)\n",
    "            except RateLimitError:\n",
    "                wait()\n",
    "            except SDKError as e:\n",
    "                if e.status_code == 429:\n",
    "                    wait()\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(\"repeat_if_hit_api_limit -> unknown error\", e)\n",
    "                wait(60)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2904/12032 [04:06<1:19:13,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract complexity from response:\n",
      "This question is assessing knowledge of chemical bonding and the octet rule, which states that atoms tend to form bonds so that they each have eight electrons in their valence shells. To answer this question, one needs to understand how to draw Lewis structures and count electrons around the central atom. This topic is typically introduced in high school chemistry courses, but may also be reviewed in undergraduate general chemistry. The addition of the non-standard compounds requires a deeper understanding of octet rule limitations.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2950/12032 [10:31<21:44:21,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract rating from response:\n",
      "The assistant's response provides a clear and concise explanation of the knowledge and skills required to solve the given problem. The response highlights the need to understand and apply the divergence theorem, compute the divergence of a vector field, set up and evaluate a volume integral, and understand three-dimensional geometry. These are all topics typically covered in undergraduate-level mathematics courses, specifically in multivariable calculus or advanced calculus. The response is helpful, relevant, and accurate in its assessment of the question's complexity.\n",
      "\n",
      "Rating: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 3000/12032 [18:01<20:46:42,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract rating from response:\n",
      "The assistant's response provides a clear and concise explanation of the question's requirements and the mathematical concept being tested. The question involves understanding rounding rules, specifically rounding to the nearest hundred, which is typically covered in middle school mathematics curricula. The response accurately identifies that the question requires evaluating pairs of numbers to determine which ones round to 1,500 when rounded to the nearest hundred. This evaluation is straightforward and does not require advanced mathematical knowledge beyond basic rounding principles.\n",
      "\n",
      "Complexity: [[middle_school]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 3001/12032 [18:08<20:06:41,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract rating from response:\n",
      "The assistant's response effectively evaluates the complexity of the question by considering the mathematical concepts involved. The question requires understanding how to handle subtraction with negative numbers, specifically the concept that subtracting a negative is equivalent to adding a positive. This concept is usually introduced and reinforced in middle school mathematics.\n",
      "\n",
      "The explanation is clear, concise, and directly relevant to the question's content. It accurately reflects the educational level at which this topic is typically taught.\n",
      "\n",
      "Complexity: [[middle_school]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3080/12032 [29:52<20:51:51,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract rating from response:\n",
      "The assistant's response provides a clear and concise explanation of the knowledge required to address the question. The response highlights that understanding the question involves knowledge of fiscal policy, aggregate demand, and macroeconomic principles, which are typically covered in undergraduate-level economics courses. The assistant objectively evaluates the complexity of the question without answering it, adhering to the instructions provided.\n",
      "\n",
      "Rating: Complexity: [[undergraduate]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3131/12032 [37:49<21:24:16,  8.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract rating from response:\n",
      "The assistant's response provides a clear and concise explanation of the knowledge and calculations required to answer the question. The response correctly identifies that the question involves understanding the straight-line method of depreciation, which is typically covered in high school accounting or finance courses. The response also mentions the mathematical operations needed (basic arithmetic and percentages), which are generally within the scope of high school mathematics.\n",
      "\n",
      "The assistant's evaluation is objective and relevant to the question's context, providing a reasonable assessment of the question's complexity.\n",
      "\n",
      "Rating: 9/10\n",
      "\n",
      "Complexity: [[high_school]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 3352/12032 [1:09:04<24:23:36, 10.12s/it]"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "import utils.prompt as prompt\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Required to purge the module cache and use the latest version after an update\n",
    "importlib.reload(prompt)\n",
    "\n",
    "difficulty = [\"middle_school\", \"high_school\", \"undergraduate\", \"postgraduate\", \"phd\"]\n",
    "ratings = list(range(1, 11, 1))\n",
    "\n",
    "invalid_complexities = 0\n",
    "invalid_ratings = 0\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def model_as_judge(client, model, index, system_prompt, user_prompt, answer):\n",
    "    global invalid_ratings\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user request displayed below. Your evaluation should consider factors such as the following all the settings in the system prompt, correspondences to the context of the user, the helpfulness, relevance and accuracy. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example:\"Rating: [[6]]\".',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Instructions for Assistant]\n",
    "                {system_prompt}\n",
    "                [End of Instructions for Assistant]\n",
    "\n",
    "                [Question]\n",
    "                {user_prompt}\n",
    "                [End of Question]\n",
    "\n",
    "                [The Start of Assistant’s Answer]\n",
    "                {answer}\n",
    "                [The End of Assistant’s Answer]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        rating = re.search(\"\\\\[\\\\[(\\\\d+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(rating)\n",
    "        rating_int = int(rating)\n",
    "        if rating_int in ratings:\n",
    "            df.at[index, \"masj_rating\"] = rating_int\n",
    "        else:\n",
    "            invalid_ratings += 1\n",
    "    except:\n",
    "        print(f\"Could not extract rating from response:\\n{response}\\n\")\n",
    "        invalid_ratings += 1\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def estimate_complextiy_with_model(client, model, index, system_prompt, user_prompt):\n",
    "    global invalid_complexities\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Question Start]\n",
    "                {user_prompt}\n",
    "                [Question End]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        complexity = re.search(\"\\\\[\\\\[(.+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(complexity)\n",
    "\n",
    "        if complexity in difficulty:\n",
    "            df.at[index, \"masj_complexity\"] = complexity\n",
    "        else:\n",
    "            invalid_complexities += 1\n",
    "    except:\n",
    "        print(f\"Could not extract complexity from response:\\n{response}\\n\")\n",
    "        invalid_complexities += 1\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "DUMP_EVERY = 100\n",
    "\n",
    "\n",
    "def estimate_dataset(df, client, get_question_from_row, get_options_from_row, out_filename):\n",
    "    if \"masj_complexity\" not in df.columns:\n",
    "        df[\"masj_complexity\"] = \"\"\n",
    "    if \"masj_rating\" not in df.columns:\n",
    "        df[\"masj_rating\"] = 0\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if df.at[index, \"masj_complexity\"] in difficulty and df.at[index, \"masj_rating\"] in ratings:\n",
    "            continue\n",
    "\n",
    "        complexity_system_prompt = f'You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity by strictly following the scale: {\", \".join(difficulty)}. You must return the complexity by strictly following this format: \"[[complexity]]\", for example: \"Complexity: [[middle_school]]\".'\n",
    "        complexity_user_prompt = prompt.get_user_prompt(get_question_from_row(row), get_options_from_row(row))\n",
    "\n",
    "        response_complexity = estimate_complextiy_with_model(\n",
    "            client, model, index, complexity_system_prompt, complexity_user_prompt\n",
    "        )\n",
    "        wait()\n",
    "\n",
    "        model_as_judge(client, model, index, complexity_system_prompt, complexity_user_prompt, response_complexity)\n",
    "        wait()\n",
    "\n",
    "        if index % DUMP_EVERY == 0:\n",
    "            df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "\n",
    "    df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "    print(\n",
    "        f\"Processed dataset {out_filename}. Total entries: {df.shape[0]}. Invalid complexities: {invalid_complexities}. Invalid ratings: {invalid_ratings}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "ORIGINAL_DATASET = \"../data/mmlu_pro_stem\"\n",
    "original_filename = f\"{ORIGINAL_DATASET}.tsv\"\n",
    "out_filename = f\"{ORIGINAL_DATASET}_w_maj_complexity.tsv\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    out_filename if os.path.isfile(out_filename) else original_filename,\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    quotechar=\"\",\n",
    "    escapechar=\"\\\\\",\n",
    ")\n",
    "# df = df.head(10)\n",
    "\n",
    "\n",
    "estimate_dataset(\n",
    "    df=df,\n",
    "    client=client,\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=lambda row: ast.literal_eval(row[\"options\"]),\n",
    "    out_filename=out_filename,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
