{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-large-2411 Choosing the \"best\" French cheese can be\n",
      "mistral-large-2411 Determining the \"best\" French cheese can\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_keys = os.environ[\"MISTRAL_API_KEYS\"]\n",
    "model = \"mistral-large-2411\"\n",
    "\n",
    "clients = [\n",
    "    Mistral(\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    for api_key in api_keys.split(\",\")\n",
    "]\n",
    "\n",
    "for client in clients:\n",
    "    # Check API is alive\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the best French cheese?\",\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    print(chat_response.model, chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep duration: 0.5\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "from mistralai import SDKError\n",
    "from openai import RateLimitError\n",
    "\n",
    "SLEEP_DURATION = 1.2\n",
    "if len(clients) == 2:\n",
    "    SLEEP_DURATION = 0.5\n",
    "if len(clients) >= 3:\n",
    "    SLEEP_DURATION = 0.2\n",
    "\n",
    "print(\"Sleep duration:\", SLEEP_DURATION)\n",
    "\n",
    "\n",
    "def wait(duration=SLEEP_DURATION):\n",
    "    sleep(duration)\n",
    "\n",
    "\n",
    "api_limit_hits_by_client_ids = {}\n",
    "\n",
    "\n",
    "def init_api_limits() -> None:\n",
    "    global api_limit_hits_by_client_ids\n",
    "\n",
    "    api_limit_hits_by_client_ids = {}\n",
    "    for i in range(len(clients)):\n",
    "        api_limit_hits_by_client_ids[i] = 0\n",
    "\n",
    "\n",
    "request_id = 0\n",
    "\n",
    "\n",
    "def repeat_if_hit_api_limit(f):  # (1)\n",
    "    def wrapper(*args, **kw):  # (2)\n",
    "        global api_limit_hits_by_client_ids\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                return f(*args, **kw)\n",
    "            except RateLimitError:\n",
    "                client_id = request_id % len(clients)\n",
    "                api_limit_hits_by_client_ids[client_id] += 1\n",
    "\n",
    "                total_hits = 0\n",
    "                for value in api_limit_hits_by_client_ids.values():\n",
    "                    total_hits += value\n",
    "\n",
    "                if (total_hits % 10) == 0:\n",
    "                    print(f\"API limit hit {total_hits} times. Details: {api_limit_hits_by_client_ids}\")\n",
    "                wait(2)\n",
    "            except SDKError as e:\n",
    "                if e.status_code == 429:\n",
    "                    client_id = request_id % len(clients)\n",
    "                    api_limit_hits_by_client_ids[client_id] += 1\n",
    "\n",
    "                    total_hits = 0\n",
    "                    for value in api_limit_hits_by_client_ids.values():\n",
    "                        total_hits += value\n",
    "\n",
    "                    if (total_hits % 10) == 0:\n",
    "                        print(f\"API limit hit {total_hits} times. Details: {api_limit_hits_by_client_ids}\")\n",
    "                    wait(2)\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(\"repeat_if_hit_api_limit -> unknown error\", e)\n",
    "                wait(60)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def query_model(messages):\n",
    "    global request_id\n",
    "    # print(request_id % len(clients))\n",
    "    client = clients[request_id % len(clients)]\n",
    "    request_id += 1\n",
    "    response = client.chat.complete(model=model, messages=messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.prompt' from '/Users/aigoncharov/dev/sktech/phi-4/utils/prompt.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if dir1 not in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "import importlib\n",
    "\n",
    "import utils.prompt as prompt\n",
    "\n",
    "# Required to purge the module cache and use the latest version after an update\n",
    "importlib.reload(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.prompt as prompt\n",
    "\n",
    "# difficulty = [\"middle_school\", \"high_school\", \"undergraduate\", \"postgraduate\", \"phd\"]\n",
    "ratings = list(range(1, 11, 1))\n",
    "\n",
    "invalid_complexities = 0\n",
    "invalid_ratings = 0\n",
    "\n",
    "\n",
    "def model_as_judge(df, index, system_prompt, user_prompt, answer):\n",
    "    global invalid_ratings\n",
    "\n",
    "    chat_response = query_model(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user request displayed below. Your evaluation should consider factors such as the following all the settings in the system prompt, correspondences to the context of the user, the helpfulness, relevance and accuracy. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example:\"Rating: [[6]]\".',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Instructions for Assistant]\n",
    "                {system_prompt}\n",
    "                [End of Instructions for Assistant]\n",
    "\n",
    "                [Question]\n",
    "                {user_prompt}\n",
    "                [End of Question]\n",
    "\n",
    "                [The Start of Assistant’s Answer]\n",
    "                {answer}\n",
    "                [The End of Assistant’s Answer]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        rating = re.search(\"\\\\[\\\\[(\\\\d+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(rating)\n",
    "        rating_int = int(rating)\n",
    "        if rating_int in ratings:\n",
    "            df.at[index, \"masj_num_rating\"] = rating_int\n",
    "        else:\n",
    "            invalid_ratings += 1\n",
    "    except:\n",
    "        print(f\"Could not extract rating from response:\\n{response}\\n\")\n",
    "        invalid_ratings += 1\n",
    "\n",
    "\n",
    "def estimate_complextiy_with_model(df, index, system_prompt, user_prompt):\n",
    "    global invalid_complexities\n",
    "\n",
    "    chat_response = query_model(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Question Start]\n",
    "                {user_prompt}\n",
    "                [Question End]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        complexity_str = re.search(\"\\\\[\\\\[(.+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(complexity_str)\n",
    "        complexity = float(complexity_str)\n",
    "\n",
    "        if complexity >= 0.0 and complexity <= 1.0:\n",
    "            df.at[index, \"masj_num_complexity\"] = complexity\n",
    "        else:\n",
    "            invalid_complexities += 1\n",
    "    except:\n",
    "        print(f\"Could not extract complexity from response:\\n{response}\\n\")\n",
    "        invalid_complexities += 1\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "DUMP_EVERY = 50\n",
    "\n",
    "\n",
    "def estimate_dataset(in_filename, out_filename, get_question_from_row, get_options_from_row):\n",
    "    if os.path.isfile(out_filename):\n",
    "        df = pd.read_csv(\n",
    "            out_filename,\n",
    "            sep=\"\\t\",\n",
    "            header=0,\n",
    "            quoting=csv.QUOTE_NONE,\n",
    "            quotechar=\"\",\n",
    "            escapechar=\"\\\\\",\n",
    "        )\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                in_filename,\n",
    "                sep=\"\\t\",\n",
    "                header=0,\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "                quotechar=\"\",\n",
    "                escapechar=\"\\\\\",\n",
    "            )\n",
    "        except:\n",
    "            df = pd.read_csv(\n",
    "                in_filename,\n",
    "                sep=\"\\t\",\n",
    "                header=0,\n",
    "            )\n",
    "\n",
    "    global invalid_complexities\n",
    "    global invalid_ratings\n",
    "    invalid_complexities = 0\n",
    "    invalid_ratings = 0\n",
    "    init_api_limits()\n",
    "\n",
    "    if \"masj_num_complexity\" not in df.columns:\n",
    "        df[\"masj_num_complexity\"] = -1.0\n",
    "    if \"masj_num_rating\" not in df.columns:\n",
    "        df[\"masj_num_rating\"] = 0\n",
    "\n",
    "    meaningful_iteration = 0\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if (\n",
    "            isinstance(df.at[index, \"masj_num_complexity\"], float)\n",
    "            and df.at[index, \"masj_num_complexity\"] >= 0.0\n",
    "            and df.at[index, \"masj_num_complexity\"] <= 1.0\n",
    "            and df.at[index, \"masj_num_rating\"] in ratings\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        meaningful_iteration += 1\n",
    "\n",
    "        complexity_system_prompt = 'You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity as a number from 0 to 1 following the following scale as a reference: middle_school - 0.0-0.2, high_school - 0.2-0.4, undergraduate - 0.4-0.6, postgraduate - 0.6-0.8, phd - 0.8-1.0. You must return the complexity by strictly following this format: \"[[complexity]]\", for example: \"Complexity: [[0.55]]\", which corresponds to the undergraduate level.'\n",
    "        complexity_user_prompt = prompt.get_user_prompt(get_question_from_row(row), get_options_from_row(row))\n",
    "        # print(complexity_user_prompt)\n",
    "\n",
    "        response_complexity = estimate_complextiy_with_model(\n",
    "            df, index, complexity_system_prompt, complexity_user_prompt\n",
    "        )\n",
    "        wait()\n",
    "\n",
    "        model_as_judge(df, index, complexity_system_prompt, complexity_user_prompt, response_complexity)\n",
    "        wait()\n",
    "\n",
    "        if meaningful_iteration % DUMP_EVERY == 0:\n",
    "            df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "            total_hits = 0\n",
    "            for value in api_limit_hits_by_client_ids.values():\n",
    "                total_hits += value\n",
    "            print(f\"Over {meaningful_iteration} iterations we hit {total_hits} API limits\")\n",
    "\n",
    "    df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "    print(\n",
    "        f\"Processed dataset {out_filename}. Total entries: {df.shape[0]}. Invalid complexities: {invalid_complexities}. Invalid ratings: {invalid_ratings}\"\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MMLU\n",
    "# import ast\n",
    "\n",
    "# estimate_dataset(\n",
    "#     in_filename=\"../data/mmlu_pro_stem_w_maj_w_entropyphi4.tsv\",\n",
    "#     out_filename=\"../data/mmlu_pro_stem_w_numerical_maj_w_entropyphi4.tsv\",\n",
    "#     get_question_from_row=lambda row: row[\"question\"],\n",
    "#     get_options_from_row=lambda row: ast.literal_eval(row[\"options\"]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1117 [00:10<3:20:47, 10.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_options_arc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> AssertionError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m---> 24\u001b[0m \u001b[43mestimate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/arc_ch_train_w_maj_complexity.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/arc_ch_train_w_numerical_maj_complexity.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_question_from_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_options_from_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_options_arc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 158\u001b[0m, in \u001b[0;36mestimate_dataset\u001b[0;34m(in_filename, out_filename, get_question_from_row, get_options_from_row)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# print(complexity_user_prompt)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m response_complexity \u001b[38;5;241m=\u001b[39m estimate_complextiy_with_model(\n\u001b[1;32m    156\u001b[0m     df, index, complexity_system_prompt, complexity_user_prompt\n\u001b[1;32m    157\u001b[0m )\n\u001b[0;32m--> 158\u001b[0m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m model_as_judge(df, index, complexity_system_prompt, complexity_user_prompt, response_complexity)\n\u001b[1;32m    161\u001b[0m wait()\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(duration)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait\u001b[39m(duration\u001b[38;5;241m=\u001b[39mSLEEP_DURATION):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ARC CH\n",
    "\n",
    "\n",
    "def get_options_arc(row):\n",
    "    try:\n",
    "        options_len = int(row[\"leng\"])\n",
    "        options_str = row[\"text\"]\n",
    "        options_str_without_newline = options_str.replace(\"\\n\", \"\")\n",
    "        options_str_without_brackets = options_str_without_newline[1:-1]\n",
    "        options_split = options_str_without_brackets.split(\"' '\")\n",
    "        # Remove leading and trailing quotes from first and last options\n",
    "        options_split[0] = options_split[0][1:]\n",
    "        options_split[-1] = options_split[-1][:-1]\n",
    "        # print(options_split, options_len)\n",
    "        assert len(options_split) == options_len\n",
    "        for option in options_split:\n",
    "            assert len(option) > 0\n",
    "        return options_split\n",
    "    except AssertionError as e:\n",
    "        print(f\"get_options_arc: {row['id']} -> AssertionError: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "estimate_dataset(\n",
    "    in_filename=\"../data/arc_ch_train_w_maj_complexity.tsv\",\n",
    "    out_filename=\"../data/arc_ch_train_w_numerical_maj_complexity.tsv\",\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=get_options_arc,\n",
    ")\n",
    "estimate_dataset(\n",
    "    in_filename=\"../data/arc_ch_test_w_maj_complexity.tsv\",\n",
    "    out_filename=\"../data/arc_ch_test_w_numerical_maj_complexity.tsv\",\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=get_options_arc,\n",
    ")\n",
    "estimate_dataset(\n",
    "    in_filename=\"../data/arc_ch_validation_w_maj_complexity.tsv\",\n",
    "    out_filename=\"../data/arc_ch_validation_w_numerical_maj_complexity.tsv\",\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=get_options_arc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SCIQ\n",
    "# from random import shuffle\n",
    "\n",
    "\n",
    "# def get_options_sciq(row):\n",
    "#     try:\n",
    "#         options_len = 4\n",
    "#         correct_option = row[\"correct\"]\n",
    "#         other_options = [row[\"incorrect1\"], row[\"incorrect2\"], row[\"incorrect3\"]]\n",
    "#         all_options = [correct_option] + other_options\n",
    "#         shuffle(all_options)\n",
    "\n",
    "#         assert len(all_options) == options_len\n",
    "#         for option in all_options:\n",
    "#             assert len(option) > 0\n",
    "#         return all_options\n",
    "#     except AssertionError as e:\n",
    "#         print(f\"get_options_sciq: {row['id']} -> AssertionError: {e}\")\n",
    "#         raise e\n",
    "\n",
    "\n",
    "# estimate_dataset(\n",
    "#     in_filename=\"../data/sciq_train.tsv\",\n",
    "#     out_filename=\"../data/sciq_train_w_maj_complexity.tsv\",\n",
    "#     get_question_from_row=lambda row: row[\"question\"],\n",
    "#     get_options_from_row=get_options_sciq,\n",
    "# )\n",
    "# estimate_dataset(\n",
    "#     in_filename=\"../data/sciq_test.tsv\",\n",
    "#     out_filename=\"../data/sciq_test_w_maj_complexity.tsv\",\n",
    "#     get_question_from_row=lambda row: row[\"question\"],\n",
    "#     get_options_from_row=get_options_sciq,\n",
    "# )\n",
    "# estimate_dataset(\n",
    "#     in_filename=\"../data/sciq_validation.tsv\",\n",
    "#     out_filename=\"../data/sciq_validation_w_maj_complexity.tsv\",\n",
    "#     get_question_from_row=lambda row: row[\"question\"],\n",
    "#     get_options_from_row=get_options_sciq,\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
