{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-large-2411 Determining the \"best\" French cheese can\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-2411\"\n",
    "\n",
    "client = Mistral(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Check API is alive\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best French cheese?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=10,\n",
    ")\n",
    "print(chat_response.model, chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "from mistralai import SDKError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def wait():\n",
    "    sleep(1.5)\n",
    "\n",
    "\n",
    "def repeat_if_hit_api_limit(f):  # (1)\n",
    "    def wrapper(*args, **kw):  # (2)\n",
    "        while True:\n",
    "            try:\n",
    "                return f(*args, **kw)\n",
    "            except RateLimitError:\n",
    "                wait()\n",
    "            except SDKError as e:\n",
    "                if e.status_code == 429:\n",
    "                    wait()\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 610/12032 [01:25<21:43,  8.76it/s]"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import utils.prompt as prompt\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Required to purge the module cache and use the latest version after an update\n",
    "importlib.reload(prompt)\n",
    "\n",
    "difficulty = [\"middle_school\", \"high_school\", \"undergraduate\", \"postgraduate\", \"phd\"]\n",
    "ratings = list(range(1, 11, 1))\n",
    "\n",
    "invalid_complexities = 0\n",
    "invalid_ratings = 0\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def model_as_judge(client, model, index, system_prompt, user_prompt, answer):\n",
    "    global invalid_ratings\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user request displayed below. Your evaluation should consider factors such as the following all the settings in the system prompt, correspondences to the context of the user, the helpfulness, relevance and accuracy. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example:\"Rating: [[6]]\".',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Instructions for Assistant]\n",
    "                {system_prompt}\n",
    "                [End of Instructions for Assistant]\n",
    "\n",
    "                [Question]\n",
    "                {user_prompt}\n",
    "                [End of Question]\n",
    "\n",
    "                [The Start of Assistant’s Answer]\n",
    "                {answer}\n",
    "                [The End of Assistant’s Answer]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        rating = re.search(\"\\\\[\\\\[(\\\\d+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(rating)\n",
    "        rating_int = int(rating)\n",
    "        if rating_int in ratings:\n",
    "            df.at[index, \"masj_rating\"] = rating_int\n",
    "        else:\n",
    "            invalid_ratings += 1\n",
    "    except:\n",
    "        print(f\"Could not extract rating from response:\\n{response}\\n\")\n",
    "        invalid_ratings += 1\n",
    "\n",
    "\n",
    "@repeat_if_hit_api_limit\n",
    "def estimate_complextiy_with_model(client, model, index, system_prompt, user_prompt):\n",
    "    global invalid_complexities\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                [Question Start]\n",
    "                {user_prompt}\n",
    "                [Question End]\n",
    "                \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = chat_response.choices[0].message.content\n",
    "    # print(response)\n",
    "\n",
    "    try:\n",
    "        complexity = re.search(\"\\\\[\\\\[(.+?)\\\\]\\\\]\", response).group(1)\n",
    "        # print(complexity)\n",
    "\n",
    "        if complexity in difficulty:\n",
    "            df.at[index, \"masj_complexity\"] = complexity\n",
    "        else:\n",
    "            invalid_complexities += 1\n",
    "    except:\n",
    "        print(f\"Could not extract complexity from response:\\n{response}\\n\")\n",
    "        invalid_complexities += 1\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "DUMP_EVERY = 100\n",
    "\n",
    "\n",
    "def estimate_dataset(df, client, get_question_from_row, get_options_from_row, out_filename):\n",
    "    if \"masj_complexity\" not in df.columns:\n",
    "        df[\"masj_complexity\"] = \"\"\n",
    "    if \"masj_rating\" not in df.columns:\n",
    "        df[\"masj_rating\"] = 0\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if df.at[index, \"masj_complexity\"] in difficulty and df.at[index, \"masj_rating\"] in ratings:\n",
    "            continue\n",
    "\n",
    "        complexity_system_prompt = f'You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity by strictly following the scale: {\", \".join(difficulty)}. You must return the complexity by strictly following this format: \"[[complexity]]\", for example: \"Complexity: [[middle_school]]\".'\n",
    "        complexity_user_prompt = prompt.get_user_prompt(get_question_from_row(row), get_options_from_row(row))\n",
    "\n",
    "        response_complexity = estimate_complextiy_with_model(\n",
    "            client, model, index, complexity_system_prompt, complexity_user_prompt\n",
    "        )\n",
    "        wait()\n",
    "\n",
    "        model_as_judge(client, model, index, complexity_system_prompt, complexity_user_prompt, response_complexity)\n",
    "        wait()\n",
    "\n",
    "        if index % DUMP_EVERY == 0:\n",
    "            df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "\n",
    "    df.to_csv(out_filename, sep=\"\\t\", quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\", index=False)\n",
    "    print(\n",
    "        f\"Processed dataset {out_filename}. Total entries: {df.shape[0]}. Invalid complexities: {invalid_complexities}. Invalid ratings: {invalid_ratings}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "DATASET = \"../data/mmlu_pro_stem\"\n",
    "out_filename = f\"{DATASET}_w_maj_complexity.tsv\"\n",
    "\n",
    "df = pd.read_csv(out_filename, sep=\"\\t\", header=0, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\"\\\\\")\n",
    "# df = df.head(10)\n",
    "\n",
    "\n",
    "estimate_dataset(\n",
    "    df=df,\n",
    "    client=client,\n",
    "    get_question_from_row=lambda row: row[\"question\"],\n",
    "    get_options_from_row=lambda row: ast.literal_eval(row[\"options\"]),\n",
    "    out_filename=out_filename,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
