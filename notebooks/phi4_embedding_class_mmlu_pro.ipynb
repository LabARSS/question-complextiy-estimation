{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75259a9b-7d86-446b-9ac9-4153eefa9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 25 06:30:03 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:9E:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              73W / 400W |  30006MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    108320      C   python                                    29998MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfabeb3-77cc-4045-8e6b-3ac1f440a00d",
   "metadata": {},
   "source": [
    "Introduces QA complexity estimation as a novel metric for optimizing LLM fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e36209-6acf-4320-9afe-960dc88295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c1f433-fb0b-4146-a9a2-5e9b022ed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "VALID_PART = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27611491-5425-43e9-a279-1e2eb24fed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"mmlu_pro_stem.tsv\"\n",
    "data_path = os.path.join(DATA_PATH, data_name)\n",
    "\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "# df = shuffle(df)\n",
    "df[\"options\"] = df[\"options\"].apply(ast.literal_eval)\n",
    "df[\"answer_index\"] = df[\"answer_index\"].apply(lambda x: str(x + 1))\n",
    "\n",
    "def enumerate_question_and_options(line):\n",
    "    enumerated_variants = \"\\n\".join(\n",
    "        f\"{i + 1}) {option}\" for i, option in enumerate(line[\"options\"])\n",
    "    )\n",
    "    return f\"{line['question']}\\n\\n{enumerated_variants}\"\n",
    "\n",
    "df[\"question_with_variants\"] = df.apply(enumerate_question_and_options, axis=1)\n",
    "\n",
    "# train_length = int((1 - VALID_PART) * df.shape[0])\n",
    "# df_train = df.iloc[:train_length].reset_index(drop=True)\n",
    "# df_valid = df.iloc[train_length:].reset_index(drop=True)\n",
    "\n",
    "# print(df_train.shape[0], df_valid.shape[0])\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce619a28-6a97-4b92-aa8d-5ac02daf6586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following criticisms of Llewellyn's distinction between the grand and formal styles of legal reasoning is the most compelling?\n",
      "\n",
      "1) There is no distinction between the two forms of legal reasoning.\n",
      "2) Judges are appointed to interpret the law, not to make it.\n",
      "3) It is misleading to pigeon-hole judges in this way.\n",
      "4) Judicial reasoning is always formal.\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(df.iloc[idx].question_with_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930a9b6-8213-47da-80f6-36d94dfd916d",
   "metadata": {},
   "source": [
    "### Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff54274-0bc3-41d4-80fe-f6a2b7682663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"microsoft/phi-4\",\n",
    "#     model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2fe5b6-3c1b-4a83-925b-4c2c6b8c78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_answers = list()\n",
    "# for idx in tqdm(range(df.shape[0])):\n",
    "#     row = df.iloc[idx]\n",
    "\n",
    "#     system_prompt = f\"You are an expert in the field of {row['category']}. Answer the questions.\"\n",
    "#     prompt = \"Choose one of the answers. Write down ONLY the NUMBER of the correct answer and nothing else.\"\n",
    "    \n",
    "#     request = (\n",
    "#         prompt\n",
    "#         + \"\\n\\n\"\n",
    "#         + row[\"question_with_variants\"]\n",
    "#     )\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": request},\n",
    "#     ]\n",
    "    \n",
    "#     outputs = pipeline(messages, max_new_tokens=1)\n",
    "#     llm_answers.append(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c14de3-c362-4a6f-bdd0-28414bcdd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"llm_answers\"] = llm_answers\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e275926f-1253-4990-bf97-531cd6be22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(df[\"answer_index\"] == df[\"llm_answers\"]) / df[\"answer_index\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0b8304-6f03-42e3-ae6f-85626fe03bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"saved_mmlu_pro_predictions.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70490b0c-21cb-46a1-ab2e-ed583bed01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b85d8a-2044-4dc7-b2aa-8384f0c73446",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f5cd5d-fdd6-44a1-88b8-c85364b3cc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a12ee5a0fe54a3cb36ce8277e4936c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=\"microsoft/phi-4\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3aab5d-f2e4-4a4f-9902-6801f5de9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/phi-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd096d-c2d4-4b6c-b915-2ec2cb7e01f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/12032 [00:01<23:11,  8.64it/s]  You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  2%|â–         | 255/12032 [00:30<17:22, 11.30it/s]"
     ]
    }
   ],
   "source": [
    "embedding_list = list()\n",
    "for idx in tqdm(range(df.shape[0])):\n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    system_prompt = f\"You are an expert in the field of {row['category']}. Answer the questions.\"\n",
    "    prompt = \"Choose one of the answers. Write down ONLY the NUMBER of the correct answer and nothing else.\"\n",
    "    \n",
    "    request = (\n",
    "        prompt\n",
    "        + \"\\n\\n\"\n",
    "        + row[\"question_with_variants\"]\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": request},\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    token_embeddings = pipeline(formatted_prompt)\n",
    "    \n",
    "    sentence_embedding = np.mean(token_embeddings[0], axis=0)\n",
    "\n",
    "    embedding_list.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176a115-ba2a-4edc-82d9-8fd1a61d5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array = np.array(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f515ac-3af3-43b3-be30-56ee56c9c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embedding_array.npy\", embedding_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd3818-9cf3-4d6a-bb1d-84a90f5abfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a0113-1cfc-44f8-93b5-47dcd55c92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98996aa-c3b0-490f-9b53-a1f321169b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
